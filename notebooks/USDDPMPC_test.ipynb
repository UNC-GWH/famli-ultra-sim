{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import nrrd\n",
    "import vtk\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import pickle\n",
    "import monai \n",
    "import glob \n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "sys.path.append('/mnt/raid/C1_ML_Analysis/source/famli-ultra-sim/')\n",
    "sys.path.append('/mnt/raid/C1_ML_Analysis/source/famli-ultra-sim/dl')\n",
    "import dl.loaders.ultrasound_dataset as ultrasound_dataset\n",
    "import dl.transforms.ultrasound_transforms as ultrasound_transforms\n",
    "import dl.nets.us_simulation_jit as us_simulation_jit\n",
    "import dl.nets.us_simu as us_simu\n",
    "\n",
    "import importlib\n",
    "\n",
    "sys.path.append('/mnt/raid/C1_ML_Analysis/source/ShapeAXI/src/')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diffusion(X):\n",
    "    num_surf = len(X)\n",
    "    specs_r = [{'type': 'scatter3d'} for _ in range(num_surf)]\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=num_surf,\n",
    "        specs=[specs_r]\n",
    "    )\n",
    "\n",
    "    for idx, x in zip(range(num_surf), X):\n",
    "        # First scatter plot\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(x=x[:,0], y=x[:,1], z=x[:,2], mode='markers', marker=dict(\n",
    "                size=2,\n",
    "                color=x[:,2],                # set color to an array/list of desired values\n",
    "                colorscale='Viridis',   # choose a colorscale\n",
    "                opacity=0.8\n",
    "            )),\n",
    "            row=1, col=idx+1\n",
    "        )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_size = 4\n",
    "dm = ultrasound_dataset.ImgPCDataModule(\n",
    "    mount_point=\"/mnt/raid/C1_ML_Analysis/simulated_data_export\",\n",
    "    csv_train=\"simulated_data_export/studies_fetus_train_train.csv\",\n",
    "    np_train=\"simulated_data_export/studies_fetus_train_train.npy\",\n",
    "    csv_valid=\"simulated_data_export/studies_fetus_train_test.csv\",\n",
    "    np_valid=\"simulated_data_export/studies_fetus_train_test.npy\",\n",
    "    csv_test=\"simulated_data_export/studies_fetus_test.csv\",\n",
    "    np_test=\"simulated_data_export/studies_fetus_test.npy\",\n",
    "    num_samples_train=5000,\n",
    "    rescale_factor=10,\n",
    "    batch_size=1\n",
    "    )\n",
    "dm.setup()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = us_simu.USDDPMPC.load_from_checkpoint(\"/mnt/raid/C1_ML_Analysis/train_output/simu_reconstruction/USDDPMPC/v0.3/epoch=85-val_loss=0.00.ckpt\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_test = dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_origin, X_end, X_PC = next(iter(dl_test))\n",
    "X = X[0:1]\n",
    "X_origin = X_origin[0:1] \n",
    "X_end = X_end[0:1]\n",
    "X_PC = X_PC[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_diffusion(X_PC.cpu().numpy())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sweeps, sweeps_tags = model.volume_sampling(X.to(device), X_origin.to(device), X_end.to(device))\n",
    "\n",
    "# x_sweeps shape is B, N, C, T, H, W. N for number of sweeps ex. torch.Size([2, 2, 1, 200, 256, 256]) \n",
    "# tags shape torch.Size([2, 2])\n",
    "with torch.no_grad():\n",
    "    batch_size = x_sweeps.shape[0]\n",
    "    Nsweeps = x_sweeps.shape[1] # Number of sweeps -> T\n",
    "\n",
    "    z = []\n",
    "    x_v = []\n",
    "\n",
    "    for n in range(Nsweeps):\n",
    "        x_sweeps_n = x_sweeps[:, n, :, :, :, :] # [BS, C, T, H, W]\n",
    "        sweeps_tags_n = sweeps_tags[:, n]\n",
    "\n",
    "        z_mu, z_sigma = model.encode(x_sweeps_n)\n",
    "        z_ = z_mu\n",
    "\n",
    "        z_ = model.attn_chunk(z_) # [BS, self.hparams.latent_channels, self.hparams.n_chunks, 64. 64]\n",
    "\n",
    "        z_ = z_.permute(0, 2, 3, 4, 1).reshape(batch_size, model.hparams.n_chunks, -1) # [BS, self.hparams.n_chunks, 64*64*self.hparams.latent_channels]\n",
    "\n",
    "        z.append(z_.unsqueeze(1))\n",
    "\n",
    "    z = torch.cat(z, dim=1) # [BS, N, self.hparams.n_chunks, 64*64*self.hparams.latent_channels]\n",
    "\n",
    "    z = model.proj(z) # [BS, N, elf.hparams.n_chunks, 1280]\n",
    "\n",
    "    # We don't need to do the trick of using the buffer for the positional encoding here, ALL the sweeps are present in validation\n",
    "    z = model.p_encoding(z)\n",
    "    z = z.view(batch_size, -1, model.hparams.embed_dim).contiguous()\n",
    "\n",
    "    pc, intermediates = model.sample(intermediate_steps=5, z=z)\n",
    "    # pc, intermediates = model.sample_wguidance(intermediate_steps=5, z=z, guidance_scale=7.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_diffusion(torch.cat(intermediates, dim=0).cpu().numpy())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pc_f, intermediates_f = model.sample(intermediate_steps=5, z=torch.zeros(1, 1, model.hparams.embed_dim, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_diffusion(torch.cat(intermediates_f, dim=0).cpu().numpy())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_guided(self, num_samples=1, guidance_scale=7.5, intermediate_steps=None, z=None):\n",
    "    intermediates = []\n",
    "\n",
    "    # Initialize random noise\n",
    "    device = self.device\n",
    "    x_t = torch.randn(num_samples, 64*64, self.hparams.input_dim, device=device)\n",
    "\n",
    "    for i, t in enumerate(self.noise_scheduler.timesteps):\n",
    "        \n",
    "        # Conditional prediction (with context)\n",
    "        x_cond = self(\n",
    "            x_t.permute(0, 2, 1).view(-1, self.hparams.input_dim, 64, 64).contiguous(),\n",
    "            timesteps=t,\n",
    "            context=z\n",
    "        )\n",
    "        x_cond = x_cond.view(-1, self.hparams.input_dim, 64*64).permute(0, 2, 1)\n",
    "\n",
    "        # Unconditional prediction (without context)\n",
    "        x_uncond = self(\n",
    "            x_t.permute(0, 2, 1).view(-1, self.hparams.input_dim, 64, 64).contiguous(),\n",
    "            timesteps=t,\n",
    "            context=torch.zeros(num_samples, 1, self.hparams.embed_dim, device=device)\n",
    "        )\n",
    "        x_uncond = x_uncond.view(-1, self.hparams.input_dim, 64*64).permute(0, 2, 1).contiguous()\n",
    "\n",
    "        # Perform classifier-free guidance\n",
    "        x_guided = x_uncond + guidance_scale * (x_cond - x_uncond)\n",
    "\n",
    "        # Update the diffusion step using guided output\n",
    "        x_t = self.noise_scheduler.step(model_output=x_guided, timestep=t, sample=x_t).prev_sample\n",
    "\n",
    "        # Save intermediate steps if needed\n",
    "        if intermediate_steps is not None and intermediate_steps > 0 and t % (self.hparams.num_train_steps//intermediate_steps) == 0:\n",
    "            intermediates.append(x_t)\n",
    "\n",
    "    return x_t, intermediates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pc_guided, intermediates_guided = sample_guided(model, intermediate_steps=5, guidance_scale=10, z=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_diffusion(torch.cat(intermediates_guided, dim=0).cpu().numpy())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
