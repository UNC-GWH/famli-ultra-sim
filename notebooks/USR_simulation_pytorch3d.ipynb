{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "113e45f5-bc1e-4650-b328-2e443fbb883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import nrrd\n",
    "import vtk\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import pickle\n",
    "import monai \n",
    "import glob \n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "sys.path.append('/mnt/famli_netapp_shared/C1_ML_Analysis/src/famli-ultra-sim/')\n",
    "sys.path.append('/mnt/famli_netapp_shared/C1_ML_Analysis/src/famli-ultra-sim/dl')\n",
    "import dl.transforms.ultrasound_transforms as ultrasound_transforms\n",
    "import dl.loaders.mr_us_dataset as mr_us_dataset\n",
    "import dl.nets.us_simulation_jit as us_simulation_jit\n",
    "import dl.nets.us_simu as us_simu\n",
    "\n",
    "import importlib\n",
    "\n",
    "from dl.nets.layers import TimeDistributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b3bc806",
   "metadata": {},
   "outputs": [],
   "source": [
    "mount_point = '/mnt/famli_netapp_shared/C1_ML_Analysis'\n",
    "               \n",
    "from shapeaxi import utils\n",
    "from shapeaxi.saxi_dataset import SaxiDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa602b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(os.path.join(mount_point, 'source/diffusion-models/blender/studies/placenta/FAM-025-0754-2.csv'))\n",
    "\n",
    "ds = SaxiDataset(df, mount_point=mount_point, surf_column='surf', transform=None, CN=True)\n",
    "dl = DataLoader(ds, batch_size=150, num_workers=2, collate_fn=utils.pad_verts_faces)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69453ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_params_df = pd.read_csv(os.path.join(mount_point, 'source/blender/simulated_data_export/probe_params.csv'))\n",
    "\n",
    "probe_directions = []\n",
    "probe_origins = []\n",
    "\n",
    "for _, row in probe_params_df.iterrows():\n",
    "    probe_params = pickle.load(open(os.path.join(mount_point, row['probe_param_fn']), 'rb'))\n",
    "\n",
    "    probe_direction = torch.tensor(probe_params['probe_direction'], dtype=torch.float32)\n",
    "    probe_origin = torch.tensor(probe_params['probe_origin'], dtype=torch.float32)\n",
    "\n",
    "    probe_directions.append(probe_direction.T)\n",
    "    probe_origins.append(probe_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0cbcdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.structures import (\n",
    "    Meshes,\n",
    "    Pointclouds,)\n",
    "\n",
    "from pytorch3d.renderer import (\n",
    "        FoVPerspectiveCameras, PerspectiveCameras, look_at_rotation, \n",
    "        RasterizationSettings, MeshRenderer, MeshRasterizer, MeshRendererWithFragments,\n",
    "        HardPhongShader, AmbientLights, TexturesVertex\n",
    ")\n",
    "from pytorch3d.ops import (sample_points_from_meshes,\n",
    "                           knn_points, \n",
    "                           knn_gather)\n",
    "\n",
    "from pytorch3d.loss import (\n",
    "    chamfer_distance,\n",
    "    point_mesh_edge_distance, \n",
    "    point_mesh_face_distance\n",
    ")\n",
    "\n",
    "\n",
    "cameras = FoVPerspectiveCameras()\n",
    "\n",
    "raster_settings = RasterizationSettings(image_size=128, blur_radius=0, faces_per_pixel=1,max_faces_per_bin=200000)        \n",
    "rasterizer = MeshRasterizer(cameras=cameras, raster_settings=raster_settings)\n",
    "lights = AmbientLights()\n",
    "renderer = MeshRenderer(rasterizer=rasterizer,shader=HardPhongShader(cameras=cameras, lights=lights))\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "renderer = renderer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c83e1c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(V, F, CN, camera_position, R):\n",
    "    # Render the input surface mesh to an image\n",
    "    textures = TexturesVertex(verts_features=CN.to(torch.float32))\n",
    "    meshes = Meshes(verts=V, faces=F, textures=textures)\n",
    "    \n",
    "    X = []\n",
    "    PF = []\n",
    "\n",
    "    # for camera_position in self.ico_verts:\n",
    "    #     camera_position = camera_position.unsqueeze(0)\n",
    "    #     R = look_at_rotation(camera_position, device=self.device)  # (1, 3, 3)\n",
    "    #     T = -torch.bmm(R.transpose(1, 2), camera_position[:,:,None])[:, :, 0]   # (1, 3)\n",
    "    #     images = self.renderer(meshes_world=meshes.clone(), R=R, T=T)        \n",
    "    #     fragments = self.renderer.rasterizer(meshes.clone())\n",
    "    #     pix_to_face = fragments.pix_to_face\n",
    "    #     zbuf = fragments.zbuf\n",
    "    #     images = torch.cat([images[:,:,:,0:3], zbuf], dim=-1)\n",
    "    #     images = images.permute(0,3,1,2)\n",
    "    #     pix_to_face = pix_to_face.permute(0,3,1,2)\n",
    "    #     X.append(images.unsqueeze(1))\n",
    "    #     PF.append(pix_to_face.unsqueeze(1))\n",
    "    \n",
    "    # camera_position = camera_position.unsqueeze(0)\n",
    "    # R = look_at_rotation(camera_position, device=V.device)  # (1, 3, 3)\n",
    "    T = -torch.bmm(R.transpose(1, 2), camera_position[:,:,None])[:, :, 0]   # (1, 3)\n",
    "    \n",
    "    images = renderer(meshes_world=meshes.clone(), R=R, T=T)        \n",
    "    fragments = renderer.rasterizer(meshes.clone())\n",
    "    pix_to_face = fragments.pix_to_face\n",
    "    zbuf = fragments.zbuf\n",
    "    images = torch.cat([images[:,:,:,0:3], zbuf], dim=-1)\n",
    "    images = images.permute(0,3,1,2)\n",
    "    pix_to_face = pix_to_face.permute(0,3,1,2)\n",
    "    X.append(images.unsqueeze(1))\n",
    "    PF.append(pix_to_face.unsqueeze(1))\n",
    "    \n",
    "    X = torch.cat(X, dim=1)\n",
    "    PF = torch.cat(PF, dim=1)        \n",
    "\n",
    "    return X, PF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10a3ec9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150, 1, 4, 128, 128]) torch.Size([150, 1, 1, 128, 128])\n",
      "torch.Size([150, 1, 4, 128, 128]) torch.Size([150, 1, 1, 128, 128])\n",
      "torch.Size([44, 1, 4, 128, 128]) torch.Size([44, 1, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "for V, F, CN in dl:\n",
    "    X, PF = render(V.cuda(), F.cuda(), CN.cuda(), probe_origins[0].cuda().unsqueeze(0), probe_directions[0].cuda().unsqueeze(0))\n",
    "    print(X.shape, PF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86381176",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_origins[0].cuda().unsqueeze(0).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
