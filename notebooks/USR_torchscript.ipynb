{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11b528a5-3ac5-4e17-af8b-23a6b5fda11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as T\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import nrrd\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# sys.path.append(\"/mnt/raid/C1_ML_Analysis/source/autoencoder/src\")\n",
    "sys.path.append(\"/mnt/raid/C1_ML_Analysis/source/famli-ultra-sim/dl/\")\n",
    "sys.path.append(\"/mnt/raid/C1_ML_Analysis/source/famli-ultra-sim/dl/nets\")\n",
    "\n",
    "from nets import diffusion, spade, lotus, cut\n",
    "from generative.inferers import DiffusionInferer\n",
    "from generative.networks.nets import DiffusionModelUNet, AutoencoderKL, VQVAE, PatchDiscriminator, MultiScalePatchDiscriminator\n",
    "from generative.networks.schedulers import DDPMScheduler\n",
    "\n",
    "import monai\n",
    "from monai.transforms import (    \n",
    "    Compose,    \n",
    "    RandFlip,\n",
    "    RandRotate,\n",
    "    CenterSpatialCrop,\n",
    "    ScaleIntensityRange,\n",
    "    RandAdjustContrast,\n",
    "    RandGaussianNoise,\n",
    "    RandGaussianSmooth\n",
    ")\n",
    "\n",
    "from loaders.ultrasound_dataset import USDatasetV2, LotusDataset\n",
    "from transforms.ultrasound_transforms import LotusEvalTransforms, LotusTrainTransforms, RealUSTrainTransformsV2, RealUSEvalTransformsV2\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef200641-635a-4e55-a7ee-b91747e12c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cut = cut.Cut.load_from_checkpoint(\"/mnt/raid/C1_ML_Analysis/train_output/ultra-sim/rendering_cut/v0.12/epoch=24-val_loss=3.43.ckpt\")\n",
    "\n",
    "\n",
    "grid, inverse_grid, mask_fan = model_cut.USR.init_grids(model_cut.hparams.grid_w, model_cut.hparams.grid_h, model_cut.hparams.center_x, model_cut.hparams.center_y, model_cut.hparams.r1, model_cut.hparams.r2, model_cut.hparams.theta)\n",
    "\n",
    "x = torch.randn(1, 1, 256, 256)\n",
    "repeats = [1,]*len(x.shape)\n",
    "repeats[0] = x.shape[0]\n",
    "grid = grid.repeat(repeats)\n",
    "inverse_grid = inverse_grid.repeat(repeats)\n",
    "mask_fan = mask_fan.repeat(repeats)\n",
    "\n",
    "USR = model_cut.USR.cuda()\n",
    "# USR.to_torchscript(file_path=\"/mnt/famli_netapp_shared/C1_ML_Analysis/src/famli-ultra-sim/trained_models/cut_v0.12-ae_v0.4_USR.pt\", method=\"trace\", example_inputs=(x.cuda(), grid.cuda(), inverse_grid.cuda(), mask_fan.cuda()))\n",
    "# model_cut.USR.hparams = model_cut.hparams\n",
    "\n",
    "# G = torch.jit.trace(model_cut.G, x)\n",
    "# G.save(\"/mnt/famli_netapp_shared/C1_ML_Analysis/src/famli-ultra-sim/trained_models/cut_v0.12-ae_v0.4_G.pt\")\n",
    "\n",
    "model_fn = \"/mnt/raid/C1_ML_Analysis/train_output/diffusionAE/extract_frames_Dataset_C_masked_resampled_256_spc075_wscores_meta_BPD01_MACFL025-7mo-9mo/v0.4/epoch=72-val_loss=0.01.ckpt\"\n",
    "AE = diffusion.AutoEncoderKL.load_from_checkpoint(model_fn).cuda()\n",
    "# AE.to_torchscript(file_path=\"/mnt/famli_netapp_shared/C1_ML_Analysis/src/famli-ultra-sim/trained_models/cut_v0.12-ae_v0.4_AE.pt\", method=\"trace\", example_inputs=x.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae1590f-d7eb-4a42-9ee7-c1dc75d1a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergedCut3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        USR = torch.jit.load(\"/mnt/famli_netapp_shared/C1_ML_Analysis/src/famli-ultra-sim/trained_models/cut_v0.12-ae_v0.4_USR.pt\")\n",
    "        G = torch.jit.load(\"/mnt/famli_netapp_shared/C1_ML_Analysis/src/famli-ultra-sim/trained_models/cut_v0.12-ae_v0.4_G.pt\")\n",
    "        AE = torch.jit.load(\"/mnt/famli_netapp_shared/C1_ML_Analysis/src/famli-ultra-sim/trained_models/cut_v0.12-ae_v0.4_AE.pt\")\n",
    "\n",
    "        self.register_module('USR', USR)\n",
    "        self.register_module('G', G)\n",
    "        self.register_module('AE', AE)\n",
    "\n",
    "        self.transform_us = T.Compose([T.Pad((0, 80, 0, 0)), T.CenterCrop(256)])\n",
    "\n",
    "    def init_grids(self, w, h, center_x, center_y, r1, r2, theta):\n",
    "        grid = self.compute_grid(w, h, center_x, center_y, r1, r2, theta)\n",
    "        inverse_grid, mask = self.compute_grid_inverse(grid)\n",
    "        grid = self.normalize_grid(grid)\n",
    "        inverse_grid = self.normalize_grid(inverse_grid)\n",
    "        \n",
    "        return  grid, inverse_grid, mask\n",
    "\n",
    "    def compute_grid(self, w, h, center_x, center_y, r1, r2, theta):\n",
    "\n",
    "        # Convert inputs to tensors\n",
    "        angles = torch.linspace(-theta, theta, w)  # Angles from -theta to theta\n",
    "        radii = torch.linspace(r1, r2, h)  # Linear space of radii\n",
    "\n",
    "        # Calculate sin and cos for all angles (broadcasting)\n",
    "        sin_angles = torch.sin(angles)\n",
    "        cos_angles = torch.cos(angles)\n",
    "\n",
    "        # Initialize the grid for intersection points\n",
    "        # Shape of grid: (h, w, 2) where 2 represents (x, y) coordinates\n",
    "        grid = torch.zeros(h, w, 2)\n",
    "\n",
    "        # Calculate intersections for each radius and angle\n",
    "        for i, radius in enumerate(radii):\n",
    "            x = (center_x + radius * sin_angles) # x coordinates for all angles at this radius\n",
    "            y = (center_y + radius * cos_angles) # y coordinates for all angles at this radius\n",
    "\n",
    "            grid[i] = torch.stack((x, y), dim=1)  # Update grid with coordinates\n",
    "\n",
    "        return grid\n",
    "        \n",
    "\n",
    "    def compute_grid_inverse(self, grid):\n",
    "\n",
    "        h, w, _ = grid.shape  # grid dimensions\n",
    "        inverse_grid = torch.zeros(h, w, 2)  # Initialize inverse grid\n",
    "        mask = torch.zeros(1, h, w)  # Initialize mask\n",
    "\n",
    "        # Iterate through each point in the grid\n",
    "        for j in range(h):\n",
    "            for i in range(w):\n",
    "                # Extract the polar coordinates (represented in the grid)\n",
    "                xi, yi = torch.round(grid[j, i]).to(torch.long)\n",
    "\n",
    "                # Place the Cartesian coordinates in the inverse grid\n",
    "                if 0 <= xi and xi < w and 0 <= yi and yi < h:\n",
    "                    inverse_grid[yi, xi] = torch.tensor([i, j])\n",
    "                    mask[0, yi, xi] = 1\n",
    "        return inverse_grid, self.morphology_close(mask.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "    def normalize_grid(self, grid):\n",
    "        h, w, _ = grid.shape  # grid dimensions\n",
    "        grid = grid / torch.tensor([h, w]) * 2.0 - 1.0\n",
    "        return grid\n",
    "\n",
    "    def dilate(self, x, kernel_size = 3):\n",
    "        kernel = torch.ones((1, 1, kernel_size, kernel_size), dtype=torch.float32)\n",
    "\n",
    "        # Apply convolution to simulate dilation\n",
    "        # We use padding=1 to ensure the output size is the same as the input size\n",
    "        output = F.conv2d(x, kernel, padding=1)\n",
    "\n",
    "        # Apply a threshold to get a binary output\n",
    "        dilated_image = (output > 0).float()\n",
    "\n",
    "        return dilated_image\n",
    "    \n",
    "    def erode(self, x, kernel_size = 3):\n",
    "        # Step 2: Erosion\n",
    "        # For erosion, invert the image and kernel, apply dilation, then invert the output\n",
    "        x = 1 - x\n",
    "        inverted_kernel = torch.ones((1, 1, kernel_size, kernel_size), dtype=torch.float32) # Same kernel as for dilation\n",
    "\n",
    "        # Apply convolution (dilation on inverted image) with padding to maintain size\n",
    "        eroded_output_inverted = F.conv2d(x, inverted_kernel, padding=1)\n",
    "\n",
    "        # Invert the result to get the final eroded (closing) result\n",
    "        eroded_image = 1 - (eroded_output_inverted > 0).float()\n",
    "\n",
    "        return eroded_image\n",
    "\n",
    "    def morphology_close(self, x, kernel_size=3):\n",
    "        return self.erode(self.dilate(x, kernel_size), kernel_size)\n",
    "\n",
    "    def forward(self, X, grid, inverse_grid, mask_fan):\n",
    "        \n",
    "        X = self.USR(X, grid, inverse_grid, mask_fan)\n",
    "        X = self.transform_us(X)\n",
    "        X = self.G(X)\n",
    "        return self.AE(X)[0]*self.transform_us(mask_fan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658844eb-e276-453a-a411-8f33bd09f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cut_v012 = MergedCut3().cuda()\n",
    "\n",
    "grid, inverse_grid, mask_fan = us_cut_v012.init_grids(256, 256, 128.0, -30.0, 20.0, 215.0, 0.7853981633974483)\n",
    "\n",
    "x = torch.randn(1, 1, 256, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491496d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repeats)\n",
    "print(grid.shape)\n",
    "print(grid.repeat(repeats).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f39dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = us_cut_v012(x.cuda(), grid.unsqueeze(0).cuda(), inverse_grid.unsqueeze(0).cuda(), mask_fan.unsqueeze(0).cuda())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8232fe-cca2-4436-8757-b104d4a9a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"default\")\n",
    "plt.imshow(x[0].permute(1,2,0).detach().cpu().numpy(), vmin=0, vmax=1)\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585b39b3-a961-4301-b633-b1fdfaac0a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cut = cut.CutLinear.load_from_checkpoint(\"/mnt/raid/C1_ML_Analysis/train_output/ultra-sim/rendering_cut/v2.0/epoch=24-val_loss=3.11.ckpt\", num_labels=332)\n",
    "\n",
    "\n",
    "grid, inverse_grid, mask_fan = model_cut.USR.init_grids(model_cut.hparams.grid_w, model_cut.hparams.grid_h, model_cut.hparams.center_x, model_cut.hparams.center_y, model_cut.hparams.r1, model_cut.hparams.r2, model_cut.hparams.theta)\n",
    "\n",
    "x = torch.randn(1, 1, 256, 256)\n",
    "repeats = [1,]*len(x.shape)\n",
    "repeats[0] = x.shape[0]\n",
    "grid = grid.repeat(repeats)\n",
    "inverse_grid = inverse_grid.repeat(repeats)\n",
    "mask_fan = mask_fan.repeat(repeats)\n",
    "\n",
    "\n",
    "cut_fn = \"/mnt/famli_netapp_shared/C1_ML_Analysis/src/famli-ultra-sim/trained_models/cutLinear_v1.0-ae_v0.4\"\n",
    "USR = model_cut.USR.eval().cuda()\n",
    "# USR.to_torchscript(file_path=cut_fn + \"_USR.pt\", method=\"trace\", example_inputs=(x.cuda(), grid.cuda(), inverse_grid.cuda(), mask_fan.cuda()))\n",
    "# model_cut.USR.hparams = model_cut.hparams\n",
    "\n",
    "G = torch.jit.trace(model_cut.G, x.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53ad3dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_fn = \"/mnt/raid/C1_ML_Analysis/train_output/ultra-sim/guided/v0.1/model.pth\"\n",
    "\n",
    "guided = AutoencoderKL(\n",
    "            spatial_dims=2,\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            num_channels=(64, 128, 256),\n",
    "            latent_channels=3,\n",
    "            num_res_blocks=2,\n",
    "            attention_levels=(False, False, False),\n",
    "            with_encoder_nonlocal_attn=False,\n",
    "            with_decoder_nonlocal_attn=False,\n",
    "        )\n",
    "\n",
    "guided.load_state_dict(torch.load(guided_fn), strict=False)\n",
    "guided.eval()\n",
    "x = torch.randint(low=0, high=11, size=(1, 1, 256, 256)).to(torch.float)\n",
    "\n",
    "guided_traced = torch.jit.trace(guided, (x,))\n",
    "guided_traced.save(os.path.splitext(guided_fn)[0] + \"_traced.pt\")\n",
    "\n",
    "# model.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f501f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
