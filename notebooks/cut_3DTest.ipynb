{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a4d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as T\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import nrrd\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# sys.path.append(\"/mnt/raid/C1_ML_Analysis/source/autoencoder/src\")\n",
    "sys.path.append(\"/mnt/raid/C1_ML_Analysis/source/famli-ultra-sim/dl/\")\n",
    "sys.path.append(\"/mnt/raid/C1_ML_Analysis/source/famli-ultra-sim/dl/nets\")\n",
    "\n",
    "from nets import cut\n",
    "from loaders import ultrasound_dataset as usd\n",
    "from transforms import ultrasound_transforms as ust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a926af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 2\n",
    "# dm = usd.Cut3DDataModule(\n",
    "#     csv_train_diffusor=\"/mnt/raid/C1_ML_Analysis/simulated_data_export/animation_export_resampled_sub_train_test.csv_\",\n",
    "#     csv_valid_diffusor=\"/mnt/raid/C1_ML_Analysis/simulated_data_export/animation_export_resampled_sub_train_test.csv_\",\n",
    "#     img_column_diffusor=\"img\",    \n",
    "#     csv_train=\"/mnt/raid/C1_ML_Analysis/CSV_files/ALL_C2_cines_gt_ga_withmeta_20221031_butterfly_train.csv\",\n",
    "#     csv_valid=\"/mnt/raid/C1_ML_Analysis/CSV_files/ALL_C2_cines_gt_ga_withmeta_20221031_butterfly_valid.csv\",\n",
    "#     img_column=\"file_path\",    \n",
    "#     num_frames=64,\n",
    "#     mount_point=\"/mnt/raid/C1_ML_Analysis/\",\n",
    "#     batch_size=batch_size,\n",
    "#     num_workers=1,    \n",
    "#     prefetch_factor=2,\n",
    "#     drop_last=False,\n",
    "#     )\n",
    "# dm.setup()\n",
    "\n",
    "# train_dl = dm.val_dataloader()\n",
    "# train_dl_iter = iter(train_dl)\n",
    "\n",
    "# batch = next(train_dl_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6875eb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_frames = 128\n",
    "dm = usd.USButterflyBlindSweepDataModule(    \n",
    "    csv_train=\"/mnt/raid/C1_ML_Analysis/CSV_files/ALL_C2_cines_gt_ga_withmeta_20221031_butterfly_train.csv\",\n",
    "    csv_valid=\"/mnt/raid/C1_ML_Analysis/CSV_files/ALL_C2_cines_gt_ga_withmeta_20221031_butterfly_valid.csv\",\n",
    "    img_column=\"file_path\",    \n",
    "    num_frames=num_frames,\n",
    "    continous_frames=True,\n",
    "    mount_point=\"/mnt/raid/C1_ML_Analysis/\",\n",
    "    batch_size=batch_size,\n",
    "    num_workers=1,    \n",
    "    prefetch_factor=2,\n",
    "    drop_last=False,\n",
    "    )\n",
    "dm.setup()\n",
    "\n",
    "train_dl = dm.train_dataloader()\n",
    "train_dl_iter = iter(train_dl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bed20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'betas': (0.9, 0.999),\n",
    "    'epochs': 200,\n",
    "    'patience': 30,\n",
    "    'steps': -1,\n",
    "    'batch_size': batch_size,\n",
    "    'conv3d': 1,\n",
    "    'probe_paths': '/mnt/raid/C1_ML_Analysis/simulated_data_export/animation_export/all_poses/frame_0001/probe_paths',\n",
    "    'diffusor': '/mnt/raid/C1_ML_Analysis/simulated_data_export/animation_export/all_poses/frame_0001.nrrd',\n",
    "    'params_csv': '/mnt/raid/C1_ML_Analysis/simulated_data_export/animation_export/shapes_intensity_map_nrrd.csv',\n",
    "    'center_y_start': -40.0,\n",
    "    'center_y_end': -20.0,\n",
    "    'r2_start': 2150.0,\n",
    "    'r2_end': 235.0,\n",
    "    'theta_start': np.pi / 5.0,\n",
    "    'theta_end': np.pi / 3.0,\n",
    "    'num_random_sweeps': 1,\n",
    "    'lambda_y': 1,\n",
    "    'create_grids': 0,\n",
    "    'n_grids': 256,\n",
    "    'num_labels': 333,  # Number of labels for the USR dictionary,\n",
    "    'grid_w': 256,\n",
    "    'grid_h': 256,\n",
    "    'center_x': 128.0,\n",
    "    'center_y': -30.0,\n",
    "    'r1': 10.0,\n",
    "    'r2': 225.0,\n",
    "    'theta': np.pi / 4.75,\n",
    "    'padding': 70,  # Padding for the simulated ultrasound\n",
    "    'num_frames': num_frames,\n",
    "    'continous_frames': True,\n",
    "}\n",
    "model = cut.CutLabel(**args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9172a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(train_dl_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97747f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_contrast(tensor, factor=1.5):\n",
    "    \"\"\"\n",
    "    Linearly increase contrast around the mean.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): Input tensor of shape (D, H, W) or (C, D, H, W)\n",
    "        factor (float): Contrast scaling factor. >1 increases contrast, <1 decreases\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Contrast-enhanced tensor\n",
    "    \"\"\"\n",
    "    mean = tensor.mean()\n",
    "    return torch.clamp((tensor - mean) * factor + mean, min=0.0, max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a6e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_grid(w=256, h=256, center_x=128, center_y=-30, r1=20.0, r2=210.0, theta=np.pi / 4.75, padding=70)\n",
    "# model.cuda()\n",
    "\n",
    "X, tags = model.volume_sampling(model.diffusor_t, model.diffusor_origin, model.diffusor_end, use_random=False)\n",
    "X = increase_contrast(X, factor=1.6)\n",
    "X = X[0]\n",
    "\n",
    "# Assume you want to overlay Y_fake[0,0] and batch[0,0]\n",
    "img1 = X[0,0,0].cpu().numpy()\n",
    "img2 = batch[0,0,0].cpu().numpy()\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Heatmap(z=np.flip(img1, axis=0), opacity=0.8, colorscale='hot'))\n",
    "fig.add_trace(go.Heatmap(z=np.flip(img2, axis=0), opacity=0.1, colorscale='ice'))\n",
    "\n",
    "fig.update_layout(height=800, width=800)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d46596",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cut.CutLabel.load_from_checkpoint('/mnt/raid/C1_ML_Analysis/train_output/Cut3d/0.5/epoch=17-val_loss=3.08.ckpt')\n",
    "model.on_fit_start()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab82410",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    X, tags = model.volume_sampling(model.diffusor_t, model.diffusor_origin, model.diffusor_end, use_random=False)\n",
    "    Y_fake = model.G(model.resize_t(X[0]))\n",
    "\n",
    "Y_fake.shape\n",
    "fig = px.imshow(Y_fake[0,0].cpu().numpy(), animation_frame=0, binary_string=True, height=800, width=800)\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
