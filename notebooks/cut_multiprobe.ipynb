{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad021602-f334-43b9-8f83-6113ce49cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as T\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import nrrd\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# sys.path.append(\"/mnt/raid/C1_ML_Analysis/source/autoencoder/src\")\n",
    "sys.path.append(\"/mnt/raid/C1_ML_Analysis/source/famli-ultra-sim/dl/\")\n",
    "sys.path.append(\"/mnt/raid/C1_ML_Analysis/source/famli-ultra-sim/dl/nets\")\n",
    "\n",
    "from nets import diffusion, spade, lotus, cut, layers, cut_G\n",
    "from loaders import ultrasound_dataset as usd\n",
    "import monai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ae5916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_size = 1\n",
    "dm = usd.CutDataModule(\n",
    "    csv_train=[\"/mnt/raid/C1_ML_Analysis/cut_multi_dataset/c1_instance_table_sonosite_resampled_file_path_extract_frames_blind_sweeps_merged_train_train_balanced.parquet\", \n",
    "        \"/mnt/raid/C1_ML_Analysis/cut_multi_dataset/c2_instance_table_clarius_resampled_file_path_extract_frames_blind_sweeps_merged_train_train_balanced.parquet\", \n",
    "        \"/mnt/raid/C1_ML_Analysis/cut_multi_dataset/extract_frames_blind_sweeps_butterfly_merged_train_train_balanced.parquet\", \n",
    "        \"/mnt/raid/C1_ML_Analysis/cut_multi_dataset/extract_frames_blind_sweeps_voluson_merged_train_train_balanced.parquet\"],\n",
    "    csv_valid=[\"/mnt/raid/C1_ML_Analysis/cut_multi_dataset/c1_instance_table_sonosite_resampled_file_path_extract_frames_blind_sweeps_merged_train_test_balanced.parquet\", \n",
    "        \"/mnt/raid/C1_ML_Analysis/cut_multi_dataset/c2_instance_table_clarius_resampled_file_path_extract_frames_blind_sweeps_merged_train_test_balanced.parquet\", \n",
    "        \"/mnt/raid/C1_ML_Analysis/cut_multi_dataset/extract_frames_blind_sweeps_butterfly_merged_train_test_balanced.parquet\", \n",
    "        \"/mnt/raid/C1_ML_Analysis/cut_multi_dataset/extract_frames_blind_sweeps_voluson_merged_train_test_balanced.parquet\"],\n",
    "    csv_test=[\"/mnt/raid/C1_ML_Analysis/cut_multi_dataset/c1_instance_table_sonosite_resampled_file_path_extract_frames_blind_sweeps_merged_test_balanced.parquet\", \n",
    "        \"/mnt/raid/C1_ML_Analysis/cut_multi_dataset/c2_instance_table_clarius_resampled_file_path_extract_frames_blind_sweeps_merged_test_balanced.parquet\", \n",
    "        \"/mnt/raid/C1_ML_Analysis/cut_multi_dataset/extract_frames_blind_sweeps_butterfly_merged_test_balanced.parquet\", \n",
    "        \"/mnt/raid/C1_ML_Analysis/cut_multi_dataset/extract_frames_blind_sweeps_voluson_merged_test_balanced.parquet\"],\n",
    "    mount_point=\"/mnt/raid/C1_ML_Analysis/\",\n",
    "    batch_size=batch_size,\n",
    "    num_workers=1,\n",
    "    img_column=\"file_path\",\n",
    "    class_column=\"pred_class\",\n",
    "    prefetch_factor=2,\n",
    "    num_samples_train=100,\n",
    "    num_samples_val=100,\n",
    "    num_samples_test=100,\n",
    "    drop_last=False\n",
    "    )\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0508fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07049ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_tensor_grid(tensor, nrow=4, title=\"Image Grid\", scale=1.0):\n",
    "    \"\"\"\n",
    "    Uses torchvision's make_grid to arrange images into a grid, then displays it with Plotly.\n",
    "\n",
    "    Args:\n",
    "        tensor: PyTorch tensor of shape [B, N, 1, H, W]\n",
    "        nrow: Number of images per row in the grid\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    B, N, C, H, W = tensor.shape\n",
    "    assert C == 1, \"Only grayscale images (C=1) are supported.\"\n",
    "\n",
    "    # Flatten batch and image dimensions\n",
    "    tensor = tensor.view(B * N, C, H, W)\n",
    "\n",
    "    # Create image grid\n",
    "    grid = make_grid(tensor, nrow=nrow, padding=2, normalize=True, scale_each=True)\n",
    "\n",
    "    # Convert to numpy for Plotly\n",
    "    grid_np = grid.permute(1,2,0).cpu().numpy()  # shape [H, W] since it's grayscale\n",
    "\n",
    "    # Determine scaled dimensions\n",
    "    height, width, C = grid_np.shape\n",
    "    display_height = int(height * scale)\n",
    "    display_width = int(width * scale)\n",
    "\n",
    "    # Plot with Plotly\n",
    "    fig = px.imshow(grid_np, color_continuous_scale='gray')\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        coloraxis_showscale=False,\n",
    "        width=display_width,\n",
    "        height=display_height\n",
    "    )\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(showticklabels=False)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec6289",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cut.CutG.load_from_checkpoint(\"/mnt/raid/C1_ML_Analysis/train_output/Cut/allvslast/allvssonosite/v0.8/last.ckpt\").eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "533cef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_dl))\n",
    "X, X_labels, Y, Y_labels = batch\n",
    "print(X.shape, Y.shape)\n",
    "# plot_tensor_grid(torch.stack(X, dim=0), nrow=4, title='Image Grid')\n",
    "px.imshow(X.squeeze(), facet_col=0, binary_string=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a518df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0a390b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    out = model(X.cuda())\n",
    "px.imshow(out.squeeze().cpu().numpy(), facet_col=0, color_continuous_scale='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb3d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_Y = monai.networks.nets.resnet18(n_input_channels=1, num_classes=1, spatial_dims=2)\n",
    "# D_Y(torch.rand(1, 1, 256, 256)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa58c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use clusters 0, 4, 6, 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
