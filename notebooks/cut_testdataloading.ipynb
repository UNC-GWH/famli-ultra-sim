{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as T\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import nrrd\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# sys.path.append(\"/mnt/raid/C1_ML_Analysis/source/autoencoder/src\")\n",
    "sys.path.append(\"/mnt/raid/C1_ML_Analysis/source/famli-ultra-sim/dl/\")\n",
    "sys.path.append(\"/mnt/raid/C1_ML_Analysis/source/famli-ultra-sim/dl/nets\")\n",
    "\n",
    "from nets import diffusion, spade, lotus, cut, layers, cut_G\n",
    "from loaders import ultrasound_dataset as usd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConditionalInstanceNorm2d(nn.Module):\n",
    "#     def __init__(self, num_features, num_classes):\n",
    "#         super().__init__()\n",
    "#         self.instance_norm = nn.InstanceNorm2d(num_features, affine=False)\n",
    "#         self.gamma_embed = nn.Embedding(num_classes, num_features)\n",
    "#         self.beta_embed = nn.Embedding(num_classes, num_features)\n",
    "#         # Initialize scale close to 1 and bias as 0.\n",
    "#         nn.init.constant_(self.gamma_embed.weight, 1)\n",
    "#         nn.init.constant_(self.beta_embed.weight, 0)\n",
    "    \n",
    "#     def forward(self, x, labels):\n",
    "#         out = self.instance_norm(x)\n",
    "#         gamma = self.gamma_embed(labels).unsqueeze(2).unsqueeze(3)\n",
    "#         beta = self.beta_embed(labels).unsqueeze(2).unsqueeze(3)\n",
    "#         return gamma * out + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cond = ConditionalInstanceNorm2d(3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_size = 8\n",
    "dm = usd.Cut3DDataModule(\n",
    "    csv_train_diffusor=\"/mnt/raid/C1_ML_Analysis/simulated_data_export/animation_export/all_poses_sub_train_train.csv\", \n",
    "    csv_valid_diffusor=\"/mnt/raid/C1_ML_Analysis/simulated_data_export/animation_export/all_poses_sub_train_test.csv\",\n",
    "    \n",
    "    mount_point=\"/mnt/raid/C1_ML_Analysis/simulated_data_export/animation_export\",\n",
    "    batch_size=batch_size,\n",
    "    num_workers=1,\n",
    "    img_column=\"file_path\",\n",
    "    class_column=\"pred_class\",\n",
    "    prefetch_factor=2,\n",
    "    drop_last=False,\n",
    "    )\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = dm.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_tensor_grid(tensor, nrow=4, title=\"Image Grid\", scale=1.0):\n",
    "    \"\"\"\n",
    "    Uses torchvision's make_grid to arrange images into a grid, then displays it with Plotly.\n",
    "\n",
    "    Args:\n",
    "        tensor: PyTorch tensor of shape [B, N, 1, H, W]\n",
    "        nrow: Number of images per row in the grid\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    B, N, C, H, W = tensor.shape\n",
    "    assert C == 1, \"Only grayscale images (C=1) are supported.\"\n",
    "\n",
    "    # Flatten batch and image dimensions\n",
    "    tensor = tensor.view(B * N, C, H, W)\n",
    "\n",
    "    # Create image grid\n",
    "    grid = make_grid(tensor, nrow=nrow, padding=2, normalize=True, scale_each=True)\n",
    "\n",
    "    # Convert to numpy for Plotly\n",
    "    grid_np = grid.permute(1,2,0).cpu().numpy()  # shape [H, W] since it's grayscale\n",
    "\n",
    "    # Determine scaled dimensions\n",
    "    height, width, C = grid_np.shape\n",
    "    display_height = int(height * scale)\n",
    "    display_width = int(width * scale)\n",
    "\n",
    "    # Plot with Plotly\n",
    "    fig = px.imshow(grid_np, color_continuous_scale='gray')\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        coloraxis_showscale=False,\n",
    "        width=display_width,\n",
    "        height=display_height\n",
    "    )\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(showticklabels=False)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))\n",
    "plot_tensor_grid(torch.stack(batch, dim=0), nrow=4, title='Image Grid')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cut_G.ConditionalGenerator(in_channels=1, features=64, residuals=9, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, patch_ids = model(input=torch.rand(1, 1, 256, 256), labels=torch.randint(low=0, high=4, size=(1,)), encode_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, _ = model(input=torch.rand(1, 1, 256, 256), labels=torch.randint(low=0, high=4, size=(1,)), encode_only=True, patch_ids=patch_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = batch\n",
    "X = torch.stack(batch, dim=0)\n",
    "        \n",
    "X_labels = torch.arange(0, len(X))\n",
    "permuted_idx = torch.randperm(X.shape[0])\n",
    "\n",
    "Y = X[permuted_idx]\n",
    "Y_labels = X_labels[permuted_idx]\n",
    "\n",
    "X_labels = X_labels.repeat(X.shape[1])\n",
    "Y_labels = Y_labels.repeat(X.shape[1])\n",
    "\n",
    "print(X.shape, X_labels.shape)\n",
    "print(Y.shape, Y_labels.shape)\n",
    "\n",
    "print(X_labels)\n",
    "print(Y_labels)\n",
    "\n",
    "X = X.view(X.shape[0]*X.shape[1], X.shape[2], X.shape[3], X.shape[4]).contiguous()\n",
    "Y = Y.view(Y.shape[0]*Y.shape[1], Y.shape[2], Y.shape[3], Y.shape[4]).contiguous()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(X, labels=Y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
