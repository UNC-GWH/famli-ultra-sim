{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641b449-2015-4da0-bf52-879abc9e37fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as T\n",
    "import numpy as np\n",
    "\n",
    "import nrrd\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# sys.path.append(\"/mnt/raid/C1_ML_Analysis/source/autoencoder/src\")\n",
    "sys.path.append(\"/mnt/raid/C1_ML_Analysis/source/famli-ultra-sim/dl/\")\n",
    "sys.path.append(\"/mnt/raid/C1_ML_Analysis/source/famli-ultra-sim/dl/nets\")\n",
    "\n",
    "from nets import diffusion\n",
    "from transforms import ultrasound_transforms as ust \n",
    "\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from nets.us_simu import VolumeSamplingBlindSweep, SweepSampling\n",
    "\n",
    "import lpips\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6820f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mount_point = '/mnt/raid/C1_ML_Analysis'\n",
    "model = diffusion.DiffusionModel.load_from_checkpoint(os.path.join(mount_point, 'train_output/diffusion/0.1/epoch=76-val_loss=0.01.ckpt'))\n",
    "model.eval()\n",
    "model = model.cuda()\n",
    "\n",
    "AE = diffusion.AutoEncoderKL.load_from_checkpoint(\"/mnt/raid/C1_ML_Analysis/train_output/diffusionAE/extract_frames_Dataset_C_masked_resampled_256_spc075_wscores_meta_BPD01_MACFL025-7mo-9mo/v0.4/epoch=72-val_loss=0.01.ckpt\")\n",
    "AE.eval()\n",
    "AE = AE.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9de2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd13b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {    \n",
    "    'probe_paths': '/mnt/raid/C1_ML_Analysis/simulated_data_export/animation_export/all_poses/frame_0001/probe_paths',\n",
    "    'diffusor_fn': '/mnt/raid/C1_ML_Analysis/simulated_data_export/animation_export/all_poses/frame_0001.nrrd',\n",
    "    'params_csv': '/mnt/raid/C1_ML_Analysis/simulated_data_export/animation_export/shapes_intensity_map_nrrd_speckel.csv',    \n",
    "    'grid_w': 256,\n",
    "    'grid_h': 256,\n",
    "    'center_x': 128.0,\n",
    "    'center_y': -40.0,\n",
    "    'r1': 20.0,\n",
    "    'r2': 255.0,\n",
    "    'theta': np.pi / 4.25,\n",
    "    'padding': 55,  # Padding for the simulated ultrasound    \n",
    "}\n",
    "ss = SweepSampling(**args).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcf2067",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x_hat = model.sample()\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Heatmap(z=np.flip(x_hat.squeeze().cpu().numpy(), axis=0), opacity=0.8, colorscale='hot'))\n",
    "# fig.add_trace(go.Heatmap(z=np.flip(img2, axis=0), opacity=0.1, colorscale='ice'))\n",
    "\n",
    "fig.update_layout(height=800, width=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791d382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "\n",
    "def ssim_loss(image, target):\n",
    "    # SSIM returns a similarity (1 = identical)\n",
    "    return 1.0 - ssim_metric(image, target)\n",
    "\n",
    "perceptual_metric = lpips.LPIPS(net='alex').to(device)\n",
    "\n",
    "def perceptual_loss(image, target):\n",
    "    # LPIPS expects 3-channel images in [-1, 1]\n",
    "    image_3c = image.repeat(1, 3, 1, 1) * 2 - 1\n",
    "    target_3c = target.repeat(1, 3, 1, 1) * 2 - 1\n",
    "    return perceptual_metric(image_3c, target_3c).mean()\n",
    "\n",
    "\n",
    "def guidance_loss(image, target, weights=(1.0, 1.0, 0.1)):\n",
    "    l1 = torch.abs(image - target).mean()\n",
    "    ssim = ssim_loss(image, target)\n",
    "    lpips_val = perceptual_loss(image, target)\n",
    "    return weights[0] * l1 + weights[1] * ssim + weights[2] * lpips_val\n",
    "\n",
    "def inference(model, scheduler, targets, guidance_scale, chunks=1, weights=(1.0, 1.0, 0.1), noise_epsilon = 0.05 ):\n",
    "    print(\"Generating image...\")\n",
    "    # noise = torch.randn(1, 1, model.hparams.image_size[1], model.hparams.image_size[0], device=targets.device)\n",
    "    base_noise = torch.randn(1, 1, model.hparams.image_size[1], model.hparams.image_size[0], device=targets.device)    \n",
    "\n",
    "    stack = []\n",
    "\n",
    "    resize_128 = ust.Resize2D((128, 128))\n",
    "    resize_256 = ust.Resize2D((256, 256))\n",
    "\n",
    "    for guide in torch.chunk(targets, chunks=chunks, dim=1):\n",
    "        guide = guide.permute(1, 0, 2, 3)  # Change to (B, C, H, W)\n",
    "        guide = resize_128(guide)\n",
    "        \n",
    "        x = base_noise + noise_epsilon * torch.randn_like(guide)\n",
    "        # x = noise.repeat(guide.shape[0], 1, 1, 1)\n",
    "        # x = torch.randn_like(guide)\n",
    "        \n",
    "        for i, t in enumerate(scheduler.timesteps):\n",
    "            with torch.no_grad():\n",
    "                noise_pred = model(x, t)\n",
    "\n",
    "            x = x.detach().requires_grad_()\n",
    "            x0 = scheduler.step(noise_pred, t, x).pred_original_sample\n",
    "\n",
    "            # Compute tweak using guidance loss gradient\n",
    "            loss = guidance_loss(x0, guide, weights) * guidance_scale\n",
    "            cond_grad = -torch.autograd.grad(loss, x, retain_graph=False)[0]\n",
    "            cond_grad = cond_grad / (cond_grad.norm() + 1e-8)\n",
    "            x = x.detach() + guidance_scale * cond_grad\n",
    "            \n",
    "            x = scheduler.step(noise_pred, t, x).prev_sample\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = AE(resize_256(x))[0]\n",
    "        stack.append(x.cpu())\n",
    "\n",
    "    # Combine chunks and remove channel dimension\n",
    "    cat = torch.cat(stack).squeeze(1)\n",
    "    print(\"Image generated!\")\n",
    "    return cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e2eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweeps, tags = ss.volume_sampling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc1c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inference_steps = 100\n",
    "guidance_scale = 15.0\n",
    "\n",
    "scheduler = DDIMScheduler(beta_start=0.0001, beta_end=0.02,\n",
    "                              beta_schedule=\"linear\")\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "\n",
    "sweeps_us = []\n",
    "for sweep in sweeps[0][2:3]:\n",
    "    sweep_us = inference(model, scheduler, sweep, guidance_scale, chunks=1, weights=(1.0, 10.0, 0.25))\n",
    "    sweeps_us.append(sweep_us)\n",
    "sweeps_us = torch.stack(sweeps_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cebe55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(sweeps_us[0], animation_frame=0, binary_string=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54457896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
