{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c28c6483-5417-418c-8770-5112c4b6ad84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'neptune-client' package has been deprecated and will be removed in the future. Install the 'neptune' package instead. For more, see https://docs.neptune.ai/setup/upgrading/\n",
      "You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import monai\n",
    "from monai.networks import nets as MNets\n",
    "from monai.networks import blocks as MBlocks\n",
    "import sys\n",
    "import pytorch_lightning as pl\n",
    "sys.path.append('/mnt/raid/C1_ML_Analysis/source/blender/famli-ultra-sim/dl/')\n",
    "\n",
    "from nets.cut_D import Discriminator\n",
    "from nets.cut_G import Generator\n",
    "from nets.cut_P import Head\n",
    "from generative.networks.nets import PatchDiscriminator\n",
    "\n",
    "from nets.lotus import UltrasoundRendering, UltrasoundRenderingLinear, UltrasoundRenderingConv1d\n",
    "import numpy as np \n",
    "from generative.losses import PatchAdversarialLoss, PerceptualLoss\n",
    "from torchvision import transforms as T\n",
    "class testNCE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.E = nets.EfficientNetBN(\"efficientnet-b0\", spatial_dims=2)\n",
    "        self.G = Generator()\n",
    "\n",
    "    def calculate_NCE_loss(self, src, tgt):\n",
    "        feat_q, patch_ids_q = self.e(tgt, encode_only=True)\n",
    "        feat_k, _ = self.E(src, encode_only=True, patch_ids=patch_ids_q)\n",
    "\n",
    "        feat_k_pool = self.H(feat_k)\n",
    "        feat_q_pool = self.H(feat_q)\n",
    "\n",
    "        total_nce_loss = 0.0\n",
    "        for f_q, f_k in zip(feat_q_pool, feat_k_pool):\n",
    "            loss = self.patch_nce_loss(f_q, f_k)\n",
    "            total_nce_loss += loss.mean()\n",
    "        return total_nce_loss / 5\n",
    "\n",
    "    def patch_nce_loss(self, feat_q, feat_k):\n",
    "        feat_k = feat_k.detach()\n",
    "        out = torch.mm(feat_q, feat_k.transpose(1, 0)) / 0.07\n",
    "        loss = self.cross_entropy_loss(out, torch.arange(0, out.size(0), dtype=torch.long, device=self.device))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fb808bd-3f14-4d21-b07f-a41a8bb7f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = testNCE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f55e19-240c-450f-b54e-ad44224861c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_q, patch_ids_q = model.G(torch.rand(4, 1, 256, 256), encode_only=True)\n",
    "# feat_k, _ = self.G(src, encode_only=True, patch_ids=patch_ids_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c902b311-4970-4b54-b5cb-57e8f766f35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 1])\n",
      "torch.Size([1024, 128])\n",
      "torch.Size([1024, 256])\n",
      "torch.Size([1024, 256])\n",
      "torch.Size([1024, 256])\n"
     ]
    }
   ],
   "source": [
    "for f in feat_q:\n",
    "    print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db162c8f-984a-45b4-9f49-45582a58433d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb21c4cf-c9f9-49ef-ba7f-2638882fa3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generative.networks.nets import SPADEAutoencoderKL\n",
    "from generative.networks.nets import SPADEDiffusionModelUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c19281a7-47e9-4464-90fb-ae05ec7fed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = SPADEAutoencoderKL(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_res_blocks=(2, 2, 2, 2),\n",
    "    num_channels=(8, 16, 32, 64),\n",
    "    attention_levels=[False, False, False, False],\n",
    "    latent_channels=8,\n",
    "    norm_num_groups=8,\n",
    "    label_nc=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b34b2619-e489-4386-9486-13112b65beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = autoencoder.encode(torch.rand(2, 1, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da8853b1-2a3a-4d41-abc6-7dd6fd3314d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([2, 1, 256, 256])\n",
      "1 torch.Size([2, 8, 256, 256])\n",
      "2 torch.Size([2, 8, 256, 256])\n",
      "3 torch.Size([2, 8, 256, 256])\n",
      "4 torch.Size([2, 8, 128, 128])\n",
      "5 torch.Size([2, 16, 128, 128])\n",
      "6 torch.Size([2, 16, 128, 128])\n",
      "7 torch.Size([2, 16, 64, 64])\n",
      "8 torch.Size([2, 32, 64, 64])\n",
      "9 torch.Size([2, 32, 64, 64])\n",
      "10 torch.Size([2, 32, 32, 32])\n",
      "11 torch.Size([2, 64, 32, 32])\n",
      "12 torch.Size([2, 64, 32, 32])\n",
      "13 torch.Size([2, 64, 32, 32])\n",
      "14 torch.Size([2, 64, 32, 32])\n",
      "15 torch.Size([2, 64, 32, 32])\n",
      "16 torch.Size([2, 64, 32, 32])\n",
      "torch.Size([2, 8, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 1, 256, 256)\n",
    "for i, l in enumerate(autoencoder.encoder.blocks):\n",
    "    print(i, x.shape)\n",
    "    x = l(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b72654eb-5fe1-455d-86e1-8e14dc9de185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_nce(x, blocks, patch_ids=None, blocks_ids=[0, 4, 7, 10, 14, 17], num_patches = 256):\n",
    "    feat = x\n",
    "    return_ids = []\n",
    "    return_feats = []\n",
    "    p_id = 0\n",
    "    for block_id, block in enumerate(blocks):\n",
    "        feat = block(feat)\n",
    "        if block_id in blocks_ids:\n",
    "            B, H, W = feat.shape[0], feat.shape[2], feat.shape[3]\n",
    "            feat_reshape = feat.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "            if patch_ids is not None:\n",
    "                patch_id = patch_ids[p_id]\n",
    "                p_id += 1\n",
    "            else:\n",
    "                patch_id = torch.randperm(feat_reshape.shape[1]) #, device=config.DEVICE\n",
    "                patch_id = patch_id[:int(min(num_patches, patch_id.shape[0]))] # .to(patch_ids.device)\n",
    "                return_ids.append(patch_id)\n",
    "            x_sample = feat_reshape[:, patch_id, :].flatten(0, 1)  # reshape(-1, x.shape[1])\n",
    "\n",
    "            return_feats.append(x_sample)\n",
    "            \n",
    "    return return_feats, return_ids, feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8082539f-51e0-455a-9289-7446e9564454",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_q, patch_ids_q, h =  encode_nce(torch.rand(4, 1, 256, 256), autoencoder.encoder.blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db21166b-17ae-4641-a4a8-cffc1e0abc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 8])\n",
      "torch.Size([1024, 16])\n",
      "torch.Size([1024, 32])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n"
     ]
    }
   ],
   "source": [
    "for f in feat_q:\n",
    "    print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51fdc771-8c43-4c36-ac56-c3f4568deb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_k, _, _ =  encode_nce(torch.rand(4, 1, 256, 256), autoencoder.encoder.blocks, patch_ids=patch_ids_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59c16af8-1d51-422a-a224-054979a4d949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 8])\n",
      "torch.Size([1024, 16])\n",
      "torch.Size([1024, 32])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n"
     ]
    }
   ],
   "source": [
    "for f in feat_k:\n",
    "    print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d3ee787-cb1c-45be-8b93-a15b31c1bca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPBlock(\n",
       "  (linear1): Linear(in_features=128, out_features=3, bias=True)\n",
       "  (linear2): Linear(in_features=3, out_features=128, bias=True)\n",
       "  (fn): GELU(approximate='none')\n",
       "  (drop1): Dropout(p=0.0, inplace=False)\n",
       "  (drop2): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MBlocks.MLPBlock(hidden_size=128, mlp_dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c32e0bc-0d90-4496-9434-eafadd054f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHeads(nn.Module):\n",
    "    def __init__(self, blocks, blocks_ids=[0, 4, 7, 10, 14, 17], in_shape=(1, 1, 128, 128)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.blocks_ids = blocks_ids\n",
    "        \n",
    "        x = torch.rand(in_shape)\n",
    "        \n",
    "        for i, layer in enumerate(blocks):\n",
    "            x = layer(x)\n",
    "            if i in blocks_ids:\n",
    "\n",
    "                mlp = nn.Sequential(*[\n",
    "                    nn.Linear(x.shape[1], 256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, 256)\n",
    "                ])\n",
    "                \n",
    "                # print(\"ProjectionHeads {i}, {shape}\".format(i=i, shape=x.shape))\n",
    "\n",
    "                setattr(self, 'mlp_%d' % i, mlp)\n",
    "                \n",
    "        \n",
    "    def forward(self, feats):\n",
    "        \n",
    "        return_feats = []\n",
    "        \n",
    "        for feat_id, feat in zip(self.blocks_ids, feats):\n",
    "            mlp = getattr(self, 'mlp_%d' % feat_id)\n",
    "            feat = mlp(feat)\n",
    "            norm = feat.pow(2).sum(1, keepdim=True).pow(1. / 2)\n",
    "            feat = feat.div(norm + 1e-7)\n",
    "            return_feats.append(feat)\n",
    "        return return_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99e134c3-e829-4031-8882-6a3dc5b146f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = ProjectionHeads(autoencoder.encoder.blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf3ebe85-f0d3-40b3-ad5d-116991306ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _Loss\n",
    "from monai.utils import LossReduction\n",
    "\n",
    "class NCELoss(_Loss):\n",
    "    \"\"\"\n",
    "    Calculates the NCELoss\n",
    "\n",
    "    Args:\n",
    "        reduction: {``\"none\"``, ``\"mean\"``, ``\"sum\"``} Specifies the reduction to apply to the output.\n",
    "        Defaults to ``\"none\"``.\n",
    "        - ``\"none\"``: no reduction will be applied.\n",
    "        - ``\"mean\"``: the sum of the output will be divided by the number of elements in the output.\n",
    "        - ``\"sum\"``: the output will be summed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        reduction: LossReduction,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(reduction=reduction)\n",
    "\n",
    "    def forward(\n",
    "        self, features_source: list, features_target: list\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            source: output of a projection head\n",
    "            target: output of a projection head\n",
    "        \"\"\"\n",
    "\n",
    "        total_nce_loss = 0.0\n",
    "        for feat_s, feat_t in zip(features_source, features_target):\n",
    "            loss = self.patch_nce_loss(feat_s, feat_t)\n",
    "            total_nce_loss += loss.mean()\n",
    "        return total_nce_loss / 5\n",
    "\n",
    "    def patch_nce_loss(self, feat_s, feat_t):\n",
    "        feat_t = feat_t.detach()\n",
    "        out = torch.mm(feat_s, feat_t.transpose(1, 0)) / 0.07\n",
    "        loss = self.criterion(out, torch.arange(0, out.size(0), dtype=torch.long))\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77c5db29-32cc-491e-8a9b-de6cd12de7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.3205, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nce_l = NCELoss(reduction='none')\n",
    "\n",
    "feat_q, patch_ids_q, _ =  encode_nce(torch.rand(4, 1, 256, 256), autoencoder.encoder.blocks)\n",
    "feat_k, _, _ =  encode_nce(torch.rand(4, 1, 256, 256), autoencoder.encoder.blocks, patch_ids=patch_ids_q)\n",
    "\n",
    "feat_q_pool = H(feat_q)\n",
    "feat_k_pool = H(feat_k)\n",
    "\n",
    "nce_l(feat_q_pool, feat_k_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c175047a-4d22-4a80-9c15-2f0ee5f67af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPADELotus(pl.LightningModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # self.D = MNets.Discriminator(in_shape=(1, 128, 128))\n",
    "        self.D = PatchDiscriminator(spatial_dims=2, num_layers_d=3, num_channels=16, in_channels=1, out_channels=1)\n",
    "\n",
    "        self.USR = UltrasoundRendering(**kwargs)\n",
    "\n",
    "        self.G = SPADEAutoencoderKL(\n",
    "                spatial_dims=2,\n",
    "                in_channels=1,\n",
    "                out_channels=1,\n",
    "                num_res_blocks=(2, 2, 2, 2),\n",
    "                num_channels=(8, 16, 32, 64),\n",
    "                attention_levels=[False, False, False, False],\n",
    "                latent_channels=8,\n",
    "                norm_num_groups=8,\n",
    "                label_nc=self.hparams.num_labels,\n",
    "            )\n",
    "        \n",
    "        self.H = ProjectionHeads(self.G.encoder.blocks)\n",
    "\n",
    "        self.perceptual_loss = PerceptualLoss(spatial_dims=2, network_type=\"alex\")\n",
    "        self.adv_loss = PatchAdversarialLoss(criterion=\"least_squares\")\n",
    "        self.nce_loss = NCELoss(reduction='none')\n",
    "        \n",
    "        self.l1 = nn.L1Loss()\n",
    "        # self.mse = nn.MSELoss()\n",
    "\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        self.transform_us = T.Compose([T.Pad((0, 80, 0, 0)), T.CenterCrop(256)])\n",
    "        \n",
    "        self.scaler_g = torch.cuda.amp.GradScaler()\n",
    "        self.scaler_d = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_gen = optim.AdamW(\n",
    "            list(self.USR.parameters()) + list(self.G.parameters()),\n",
    "            # self.USR.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            betas=self.hparams.betas,\n",
    "            weight_decay=self.hparams.weight_decay            \n",
    "        )\n",
    "        opt_disc = optim.AdamW(\n",
    "            self.D_Y.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            betas=self.hparams.betas,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )        \n",
    "        opt_head = optim.AdamW(\n",
    "            self.H.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            betas=self.hparams.betas,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "\n",
    "        return [opt_gen, opt_disc, opt_head]\n",
    "    \n",
    "    # This is only called during inference time to set a custom grid\n",
    "    def init_grid(self, w, h, center_x, center_y, r1, r2, theta):\n",
    "        grid = self.USR.compute_grid(w, h, center_x, center_y, r1, r2, theta)\n",
    "        inverse_grid, mask = self.USR.compute_grid_inverse(grid)\n",
    "        \n",
    "        self.USR.grid = self.USR.normalize_grid(grid)\n",
    "        self.USR.inverse_grid = self.USR.normalize_grid(inverse_grid)\n",
    "        self.USR.mask_fan = mask\n",
    "\n",
    "    def forward(self, X):\n",
    "        M = self.transform_us(self.USR.mask_fan)\n",
    "        S = self.one_hot(self.transform_us(X))*M\n",
    "        X = self.transform_us(self.USR(X))\n",
    "        return self.G(X, S)*M\n",
    "\n",
    "    # def scheduler_step(self):\n",
    "    #     self.scheduler_disc.step()\n",
    "    #     self.scheduler_gen.step()\n",
    "    #     self.scheduler_mlp.step()\n",
    "\n",
    "    def set_requires_grad(self, nets, requires_grad=False):\n",
    "        \"\"\"Set requies_grad=Fasle for all the networks to avoid unnecessary computations\n",
    "        Parameters:\n",
    "            nets (network list)   -- a list of networks\n",
    "            requires_grad (bool)  -- whether the networks require gradients or not\n",
    "        \"\"\"\n",
    "        if not isinstance(nets, list):\n",
    "            nets = [nets]\n",
    "        for net in nets:\n",
    "            if net is not None:\n",
    "                for param in net.parameters():\n",
    "                    param.requires_grad = requires_grad\n",
    "\n",
    "    def on_train_start(self):\n",
    "\n",
    "        # Define the file names directly without using out_dir\n",
    "        grid_t_file = 'grid_t.pt'\n",
    "        inverse_grid_t_file = 'inverse_grid_t.pt'\n",
    "        mask_fan_t_file = 'mask_fan_t.pt'\n",
    "\n",
    "        if self.hparams.create_grids or not os.path.exists(grid_t_file):\n",
    "            grid_tensor = []\n",
    "            inverse_grid_t = []\n",
    "            mask_fan_t = []\n",
    "\n",
    "            for i in range(self.hparams.n_grids):\n",
    "\n",
    "                grid_w, grid_h = self.hparams.grid_w, self.hparams.grid_h\n",
    "                center_x = self.hparams.center_x\n",
    "                r1 = self.hparams.r1\n",
    "\n",
    "                center_y = self.hparams.center_y_start + (self.hparams.center_y_end - self.hparams.center_y_start) * (torch.rand(1))\n",
    "                r2 = self.hparams.r2_start + ((self.hparams.r2_end - self.hparams.r2_start) * torch.rand(1)).item()\n",
    "                theta = self.hparams.theta_start + ((self.hparams.theta_end - self.hparams.theta_start) * torch.rand(1)).item()\n",
    "                \n",
    "                grid, inverse_grid, mask = self.USR.init_grids(grid_w, grid_h, center_x, center_y, r1, r2, theta)\n",
    "\n",
    "                grid_tensor.append(grid.unsqueeze(dim=0))\n",
    "                inverse_grid_t.append(inverse_grid.unsqueeze(dim=0))\n",
    "                mask_fan_t.append(mask.unsqueeze(dim=0))\n",
    "\n",
    "            self.grid_t = torch.cat(grid_tensor).to(self.device)\n",
    "            self.inverse_grid_t = torch.cat(inverse_grid_t).to(self.device)\n",
    "            self.mask_fan_t = torch.cat(mask_fan_t).to(self.device)\n",
    "\n",
    "            # Save tensors directly to the current directory\n",
    "            \n",
    "            torch.save(self.grid_t, grid_t_file)\n",
    "            torch.save(self.inverse_grid_t, inverse_grid_t_file)\n",
    "            torch.save(self.mask_fan_t, mask_fan_t_file)\n",
    "\n",
    "            # print(\"Grids SAVED!\")\n",
    "            # print(self.grid_t.shape, self.inverse_grid_t.shape, self.mask_fan_t.shape)\n",
    "        \n",
    "        else:\n",
    "            # Load tensors directly from the current directory\n",
    "            self.grid_t = torch.load(grid_t_file).to(self.device)\n",
    "            self.inverse_grid_t = torch.load(inverse_grid_t_file).to(self.device)\n",
    "            self.mask_fan_t = torch.load(mask_fan_t_file).to(self.device)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "\n",
    "        # Y is the real ultrasound\n",
    "        labeled, Y = train_batch\n",
    "        X_x = labeled['img']\n",
    "        X_s = labeled['seg']\n",
    "\n",
    "        opt_gen, opt_disc, opt_head = self.optimizers()\n",
    "\n",
    "        grid_idx = torch.randint(low=0, high=self.hparams.n_grids - 1, size=(X_s.shape[0],))\n",
    "        \n",
    "        grid = self.grid_t[grid_idx]\n",
    "        inverse_grid = self.inverse_grid_t[grid_idx]\n",
    "        mask_fan = self.mask_fan_t[grid_idx]\n",
    "\n",
    "        X_ = self.USR(X_s, grid=grid, inverse_grid=inverse_grid, mask_fan=mask_fan)\n",
    "        X_s = X_s*mask_fan\n",
    "        X_ = X_.detach()\n",
    "        X = self.transform_us(X_)\n",
    "        X_s = self.transform_us(X_s)\n",
    "        \n",
    "        labels = one_hot(X_s, self.hparams.num_labels).to(self.device)\n",
    "        \n",
    "        with autocast(enabled=True):\n",
    "            \n",
    "            feat_q, patch_ids_q, h = self.encode_nce(X, self.G.encoder.blocks)\n",
    "            z_mu, z_sigma = self.quant_conv(h)\n",
    "            z = self.sampling(z_mu, z_sigma)\n",
    "            Y_fake = self.G.decode(z, labels)\n",
    "            \n",
    "            recons_loss = self.l1(Y_fake, X_)\n",
    "            \n",
    "            feat_k, _, _ = self.encode_nce(Y, self.G.encoder.blocks, patch_ids=patch_ids_q)\n",
    "            \n",
    "            feat_q_pool = self.H(feat_q)\n",
    "            feat_k_pool = self.H(feat_k)\n",
    "            \n",
    "            # Y_fake, z_mu, z_sigma = self.G(X, labels)\n",
    "            \n",
    "            p_loss = self.perceptual_loss(Y_fake.float(), Y.float())\n",
    "            \n",
    "            kl_loss = 0.5 * torch.sum(z_mu.pow(2) + z_sigma.pow(2) - torch.log(z_sigma.pow(2)) - 1, dim=[1, 2, 3])\n",
    "            kl_loss = torch.sum(kl_loss) / kl_loss.shape[0]\n",
    "\n",
    "            logits_fake = self.D(Y_fake.contiguous().float())[-1]\n",
    "            a_loss = self.adv_loss(logits_fake, target_is_real=True, for_discriminator=False)\n",
    "            \n",
    "            nce_loss = self.nce_loss(feat_q_pool, feat_k_pool)\n",
    "            \n",
    "            loss_g = self.hparams.recons_weight*recons_loss + self.hparams.nce_weight*nce_loss + self.hparams.adv_weight * a_loss + (self.hparams.kl_weight * kl_loss) + (self.hparams.perceptual_weight * p_loss)\n",
    "\n",
    "            \n",
    "        # update D\n",
    "        if self.current_epoch < self.hparams.warm_up_epochs:\n",
    "            with autocast(enabled=True):\n",
    "                self.set_requires_grad(self.D, True)\n",
    "                opt_disc.zero_grad(set_to_none=True)\n",
    "                \n",
    "                loss_d = self.compute_D_loss(Y, Y_fake)\n",
    "                \n",
    "                scaler_d.scale(loss_d).backward()\n",
    "                scaler_d.step(opt_disc)\n",
    "                scaler_d.update()\n",
    "                self.set_requires_grad(self.D, False)\n",
    "\n",
    "        # update G\n",
    "        opt_gen.zero_grad()\n",
    "        opt_head.zero_grad()\n",
    "        \n",
    "        opt_gen.zero_grad(set_to_none=True)\n",
    "        scaler_g.scale(loss_g).backward()\n",
    "        scaler_g.step(opt_gen)\n",
    "        scaler_g.update()\n",
    "\n",
    "        opt_gen.step()\n",
    "        opt_head.step()\n",
    "        \n",
    "        self.log(\"train_loss_g\", loss_g)\n",
    "        self.log(\"train_loss_d\", loss_d)\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "\n",
    "        labeled, Y = val_batch\n",
    "        X_x = labeled['img']\n",
    "        X_s = labeled['seg']\n",
    "        \n",
    "        grid_idx = torch.randint(low=0, high=self.hparams.n_grids - 1, size=(X_s.shape[0],))\n",
    "        \n",
    "        grid = self.grid_t[grid_idx]\n",
    "        inverse_grid = self.inverse_grid_t[grid_idx]\n",
    "        mask_fan = self.mask_fan_t[grid_idx]\n",
    "\n",
    "        X_ = self.USR(X_s, grid=grid, inverse_grid=inverse_grid, mask_fan=mask_fan)\n",
    "        X_s = X_s*mask_fan\n",
    "        X = self.transform_us(X_)\n",
    "        X_s = self.transform_us(X_s)\n",
    "        \n",
    "        \n",
    "        labels = one_hot(X_s, self.hparams.num_labels).to(self.device)\n",
    "        \n",
    "        with autocast(enabled=True):\n",
    "            \n",
    "            feat_q, patch_ids_q, h = self.encode_nce(X, self.G.encoder.blocks)\n",
    "            z_mu, z_sigma = self.quant_conv(h)\n",
    "            z = self.sampling(z_mu, z_sigma)\n",
    "            Y_fake = self.G.decode(z, labels)\n",
    "            \n",
    "            recons_loss = self.l1(Y_fake, X_)\n",
    "            \n",
    "            feat_k, _, _ = self.encode_nce(Y, self.G.encoder.blocks, patch_ids=patch_ids_q)\n",
    "            \n",
    "            feat_q_pool = self.H(feat_q)\n",
    "            feat_k_pool = self.H(feat_k)\n",
    "            \n",
    "            p_loss = self.perceptual_loss(Y_fake.float(), Y.float())\n",
    "            \n",
    "            kl_loss = 0.5 * torch.sum(z_mu.pow(2) + z_sigma.pow(2) - torch.log(z_sigma.pow(2)) - 1, dim=[1, 2, 3])\n",
    "            kl_loss = torch.sum(kl_loss) / kl_loss.shape[0]\n",
    "\n",
    "            logits_fake = self.D(Y_fake.contiguous().float())[-1]\n",
    "            a_loss = self.adv_loss(logits_fake, target_is_real=True, for_discriminator=False)\n",
    "            \n",
    "            nce_loss = self.nce_loss(feat_q_pool, feat_k_pool)\n",
    "            \n",
    "            loss_g = self.hparams.recons_weight*recons_loss + self.hparams.nce_weight*nce_loss + self.hparams.adv_weight * a_loss + (self.hparams.kl_weight * kl_loss) + (self.hparams.perceptual_weight * p_loss)\n",
    "\n",
    "        self.log(\"val_loss\", loss_g, sync_dist=True)\n",
    "        \n",
    "    def quant_conv(self, h):\n",
    "        z_mu = self.G.quant_conv_mu(h)\n",
    "        z_log_var = self.G.quant_conv_log_sigma(h)\n",
    "        z_log_var = torch.clamp(z_log_var, -30.0, 20.0)\n",
    "        z_sigma = torch.exp(z_log_var / 2)\n",
    "\n",
    "        return z_mu, z_sigma\n",
    "    \n",
    "    def compute_D_loss(self, Y, Y_fake):\n",
    "        logits_fake = self.D(Y_fake.contiguous().detach())[-1]\n",
    "        loss_d_fake = self.adv_loss(logits_fake, target_is_real=False, for_discriminator=True)\n",
    "        logits_real = self.D(Y.contiguous().detach())[-1]\n",
    "        loss_d_real = self.adv_loss(logits_real, target_is_real=True, for_discriminator=True)\n",
    "        discriminator_loss = (loss_d_fake + loss_d_real) * 0.5\n",
    "\n",
    "        loss_d = self.hparams.adv_weight * discriminator_loss\n",
    "        \n",
    "        return loss_d\n",
    "    \n",
    "    def encode_nce(self, x, blocks, patch_ids=None, blocks_ids=[0, 4, 7, 10, 14, 17], num_patches = 256):\n",
    "        feat = x\n",
    "        return_ids = []\n",
    "        return_feats = []\n",
    "        p_id = 0\n",
    "\n",
    "        for block_id, block in enumerate(blocks):\n",
    "            feat = block(feat)\n",
    "            if block_id in blocks_ids:\n",
    "                feat_reshape = feat.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "                if patch_ids is not None:\n",
    "                    patch_id = patch_ids[p_id]\n",
    "                    p_id += 1\n",
    "                else:\n",
    "                    patch_id = torch.randperm(feat_reshape.shape[1]) #, device=config.DEVICE\n",
    "                    patch_id = patch_id[:int(min(num_patches, patch_id.shape[0]))] # .to(patch_ids.device)\n",
    "                    return_ids.append(patch_id)\n",
    "                x_sample = feat_reshape[:, patch_id, :].flatten(0, 1)  # reshape(-1, x.shape[1])\n",
    "\n",
    "                return_feats.append(x_sample)\n",
    "\n",
    "        return return_feats, return_ids, feat\n",
    "    \n",
    "    def one_hot(self, input_label):\n",
    "        # One hot encoding function for the labels\n",
    "        shape_ = list(input_label.shape)\n",
    "        shape_[1] = label_nc\n",
    "        label_out = torch.zeros(shape_)\n",
    "        for channel in range(self.hparams.num_labels):\n",
    "            label_out[:, channel, ...] = input_label[:, 0, ...] == channel\n",
    "        return label_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f53823b-a524-4b8e-aada-b675caca1bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_model = SPADELotus(num_labels=330, grid_w=128, grid_h=128, center_x=64.0, center_y=-15, r1=20.0, r2=125.0, theta=np.pi/6, alpha_coeff_boundary_map=0.1, beta_coeff_scattering=10, tgc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20292854-c486-41fe-9005-14846dc66dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SPADELotus(\n",
       "  (D): PatchDiscriminator(\n",
       "    (initial_conv): Convolution(\n",
       "      (conv): Conv2d(1, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (adn): ADN(\n",
       "        (D): Dropout(p=0.0, inplace=False)\n",
       "        (A): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "    (0): Convolution(\n",
       "      (conv): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (adn): ADN(\n",
       "        (N): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (D): Dropout(p=0.0, inplace=False)\n",
       "        (A): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "    (1): Convolution(\n",
       "      (conv): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (adn): ADN(\n",
       "        (N): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (D): Dropout(p=0.0, inplace=False)\n",
       "        (A): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "    (2): Convolution(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (adn): ADN(\n",
       "        (N): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (D): Dropout(p=0.0, inplace=False)\n",
       "        (A): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "    (final_conv): Convolution(\n",
       "      (conv): Conv2d(128, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (USR): UltrasoundRendering(\n",
       "    (loss): L1Loss()\n",
       "  )\n",
       "  (G): SPADEAutoencoderKL(\n",
       "    (encoder): Encoder(\n",
       "      (blocks): ModuleList(\n",
       "        (0): Convolution(\n",
       "          (conv): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (1-2): 2 x ResBlock(\n",
       "          (norm1): GroupNorm(8, 8, eps=1e-06, affine=True)\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (norm2): GroupNorm(8, 8, eps=1e-06, affine=True)\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (nin_shortcut): Identity()\n",
       "        )\n",
       "        (3): Downsample(\n",
       "          (conv): Convolution(\n",
       "            (conv): Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (4): ResBlock(\n",
       "          (norm1): GroupNorm(8, 8, eps=1e-06, affine=True)\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (norm2): GroupNorm(8, 16, eps=1e-06, affine=True)\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (nin_shortcut): Convolution(\n",
       "            (conv): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (5): ResBlock(\n",
       "          (norm1): GroupNorm(8, 16, eps=1e-06, affine=True)\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (norm2): GroupNorm(8, 16, eps=1e-06, affine=True)\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (nin_shortcut): Identity()\n",
       "        )\n",
       "        (6): Downsample(\n",
       "          (conv): Convolution(\n",
       "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (7): ResBlock(\n",
       "          (norm1): GroupNorm(8, 16, eps=1e-06, affine=True)\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (norm2): GroupNorm(8, 32, eps=1e-06, affine=True)\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (nin_shortcut): Convolution(\n",
       "            (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (8): ResBlock(\n",
       "          (norm1): GroupNorm(8, 32, eps=1e-06, affine=True)\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (norm2): GroupNorm(8, 32, eps=1e-06, affine=True)\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (nin_shortcut): Identity()\n",
       "        )\n",
       "        (9): Downsample(\n",
       "          (conv): Convolution(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (10): ResBlock(\n",
       "          (norm1): GroupNorm(8, 32, eps=1e-06, affine=True)\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (norm2): GroupNorm(8, 64, eps=1e-06, affine=True)\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (nin_shortcut): Convolution(\n",
       "            (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (11-12): 2 x ResBlock(\n",
       "          (norm1): GroupNorm(8, 64, eps=1e-06, affine=True)\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (norm2): GroupNorm(8, 64, eps=1e-06, affine=True)\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (nin_shortcut): Identity()\n",
       "        )\n",
       "        (13): AttentionBlock(\n",
       "          (norm): GroupNorm(8, 64, eps=1e-06, affine=True)\n",
       "          (to_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_v): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (proj_attn): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (14): ResBlock(\n",
       "          (norm1): GroupNorm(8, 64, eps=1e-06, affine=True)\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (norm2): GroupNorm(8, 64, eps=1e-06, affine=True)\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (nin_shortcut): Identity()\n",
       "        )\n",
       "        (15): GroupNorm(8, 64, eps=1e-06, affine=True)\n",
       "        (16): Convolution(\n",
       "          (conv): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): SPADEDecoder(\n",
       "      (blocks): ModuleList(\n",
       "        (0): Convolution(\n",
       "          (conv): Conv2d(8, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (1): SPADEResBlock(\n",
       "          (norm1): SPADE(\n",
       "            (param_free_norm): ADN(\n",
       "              (N): GroupNorm(8, 64, eps=1e-05, affine=False)\n",
       "            )\n",
       "            (mlp_shared): Convolution(\n",
       "              (conv): Conv2d(330, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (A): LeakyReLU(negative_slope=0.01)\n",
       "              )\n",
       "            )\n",
       "            (mlp_gamma): Convolution(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "            (mlp_beta): Convolution(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (norm2): SPADE(\n",
       "            (param_free_norm): ADN(\n",
       "              (N): GroupNorm(8, 64, eps=1e-05, affine=False)\n",
       "            )\n",
       "            (mlp_shared): Convolution(\n",
       "              (conv): Conv2d(330, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (A): LeakyReLU(negative_slope=0.01)\n",
       "              )\n",
       "            )\n",
       "            (mlp_gamma): Convolution(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "            (mlp_beta): Convolution(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (nin_shortcut): Identity()\n",
       "        )\n",
       "        (2): AttentionBlock(\n",
       "          (norm): GroupNorm(8, 64, eps=1e-06, affine=True)\n",
       "          (to_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_v): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (proj_attn): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (3-5): 3 x SPADEResBlock(\n",
       "          (norm1): SPADE(\n",
       "            (param_free_norm): ADN(\n",
       "              (N): GroupNorm(8, 64, eps=1e-05, affine=False)\n",
       "            )\n",
       "            (mlp_shared): Convolution(\n",
       "              (conv): Conv2d(330, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (A): LeakyReLU(negative_slope=0.01)\n",
       "              )\n",
       "            )\n",
       "            (mlp_gamma): Convolution(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "            (mlp_beta): Convolution(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (norm2): SPADE(\n",
       "            (param_free_norm): ADN(\n",
       "              (N): GroupNorm(8, 64, eps=1e-05, affine=False)\n",
       "            )\n",
       "            (mlp_shared): Convolution(\n",
       "              (conv): Conv2d(330, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (A): LeakyReLU(negative_slope=0.01)\n",
       "              )\n",
       "            )\n",
       "            (mlp_gamma): Convolution(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "            (mlp_beta): Convolution(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (nin_shortcut): Identity()\n",
       "        )\n",
       "        (6): Upsample(\n",
       "          (conv): Convolution(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (7): SPADEResBlock(\n",
       "          (norm1): SPADE(\n",
       "            (param_free_norm): ADN(\n",
       "              (N): GroupNorm(8, 64, eps=1e-05, affine=False)\n",
       "            )\n",
       "            (mlp_shared): Convolution(\n",
       "              (conv): Conv2d(330, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (A): LeakyReLU(negative_slope=0.01)\n",
       "              )\n",
       "            )\n",
       "            (mlp_gamma): Convolution(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "            (mlp_beta): Convolution(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (norm2): SPADE(\n",
       "            (param_free_norm): ADN(\n",
       "              (N): GroupNorm(8, 32, eps=1e-05, affine=False)\n",
       "            )\n",
       "            (mlp_shared): Convolution(\n",
       "              (conv): Conv2d(330, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (A): LeakyReLU(negative_slope=0.01)\n",
       "              )\n",
       "            )\n",
       "            (mlp_gamma): Convolution(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "            (mlp_beta): Convolution(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (nin_shortcut): Convolution(\n",
       "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (8): SPADEResBlock(\n",
       "          (norm1): SPADE(\n",
       "            (param_free_norm): ADN(\n",
       "              (N): GroupNorm(8, 32, eps=1e-05, affine=False)\n",
       "            )\n",
       "            (mlp_shared): Convolution(\n",
       "              (conv): Conv2d(330, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (A): LeakyReLU(negative_slope=0.01)\n",
       "              )\n",
       "            )\n",
       "            (mlp_gamma): Convolution(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "            (mlp_beta): Convolution(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (norm2): SPADE(\n",
       "            (param_free_norm): ADN(\n",
       "              (N): GroupNorm(8, 32, eps=1e-05, affine=False)\n",
       "            )\n",
       "            (mlp_shared): Convolution(\n",
       "              (conv): Conv2d(330, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (A): LeakyReLU(negative_slope=0.01)\n",
       "              )\n",
       "            )\n",
       "            (mlp_gamma): Convolution(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "            (mlp_beta): Convolution(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (nin_shortcut): Identity()\n",
       "        )\n",
       "        (9): Upsample(\n",
       "          (conv): Convolution(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (10): SPADEResBlock(\n",
       "          (norm1): SPADE(\n",
       "            (param_free_norm): ADN(\n",
       "              (N): GroupNorm(8, 32, eps=1e-05, affine=False)\n",
       "            )\n",
       "            (mlp_shared): Convolution(\n",
       "              (conv): Conv2d(330, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (A): LeakyReLU(negative_slope=0.01)\n",
       "              )\n",
       "            )\n",
       "            (mlp_gamma): Convolution(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "            (mlp_beta): Convolution(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (norm2): SPADE(\n",
       "            (param_free_norm): ADN(\n",
       "              (N): GroupNorm(8, 16, eps=1e-05, affine=False)\n",
       "            )\n",
       "            (mlp_shared): Convolution(\n",
       "              (conv): Conv2d(330, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (A): LeakyReLU(negative_slope=0.01)\n",
       "              )\n",
       "            )\n",
       "            (mlp_gamma): Convolution(\n",
       "              (conv): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "            (mlp_beta): Convolution(\n",
       "              (conv): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (nin_shortcut): Convolution(\n",
       "            (conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (11): SPADEResBlock(\n",
       "          (norm1): SPADE(\n",
       "            (param_free_norm): ADN(\n",
       "              (N): GroupNorm(8, 16, eps=1e-05, affine=False)\n",
       "            )\n",
       "            (mlp_shared): Convolution(\n",
       "              (conv): Conv2d(330, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (A): LeakyReLU(negative_slope=0.01)\n",
       "              )\n",
       "            )\n",
       "            (mlp_gamma): Convolution(\n",
       "              (conv): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "            (mlp_beta): Convolution(\n",
       "              (conv): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (norm2): SPADE(\n",
       "            (param_free_norm): ADN(\n",
       "              (N): GroupNorm(8, 16, eps=1e-05, affine=False)\n",
       "            )\n",
       "            (mlp_shared): Convolution(\n",
       "              (conv): Conv2d(330, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (A): LeakyReLU(negative_slope=0.01)\n",
       "              )\n",
       "            )\n",
       "            (mlp_gamma): Convolution(\n",
       "              (conv): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "            (mlp_beta): Convolution(\n",
       "              (conv): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (nin_shortcut): Identity()\n",
       "        )\n",
       "        (12): Upsample(\n",
       "          (conv): Convolution(\n",
       "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (13): SPADEResBlock(\n",
       "          (norm1): SPADE(\n",
       "            (param_free_norm): ADN(\n",
       "              (N): GroupNorm(8, 16, eps=1e-05, affine=False)\n",
       "            )\n",
       "            (mlp_shared): Convolution(\n",
       "              (conv): Conv2d(330, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (A): LeakyReLU(negative_slope=0.01)\n",
       "              )\n",
       "            )\n",
       "            (mlp_gamma): Convolution(\n",
       "              (conv): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "            (mlp_beta): Convolution(\n",
       "              (conv): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (norm2): SPADE(\n",
       "            (param_free_norm): ADN(\n",
       "              (N): GroupNorm(8, 8, eps=1e-05, affine=False)\n",
       "            )\n",
       "            (mlp_shared): Convolution(\n",
       "              (conv): Conv2d(330, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (A): LeakyReLU(negative_slope=0.01)\n",
       "              )\n",
       "            )\n",
       "            (mlp_gamma): Convolution(\n",
       "              (conv): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "            (mlp_beta): Convolution(\n",
       "              (conv): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (nin_shortcut): Convolution(\n",
       "            (conv): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (14): SPADEResBlock(\n",
       "          (norm1): SPADE(\n",
       "            (param_free_norm): ADN(\n",
       "              (N): GroupNorm(8, 8, eps=1e-05, affine=False)\n",
       "            )\n",
       "            (mlp_shared): Convolution(\n",
       "              (conv): Conv2d(330, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (A): LeakyReLU(negative_slope=0.01)\n",
       "              )\n",
       "            )\n",
       "            (mlp_gamma): Convolution(\n",
       "              (conv): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "            (mlp_beta): Convolution(\n",
       "              (conv): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (norm2): SPADE(\n",
       "            (param_free_norm): ADN(\n",
       "              (N): GroupNorm(8, 8, eps=1e-05, affine=False)\n",
       "            )\n",
       "            (mlp_shared): Convolution(\n",
       "              (conv): Conv2d(330, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (A): LeakyReLU(negative_slope=0.01)\n",
       "              )\n",
       "            )\n",
       "            (mlp_gamma): Convolution(\n",
       "              (conv): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "            (mlp_beta): Convolution(\n",
       "              (conv): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (nin_shortcut): Identity()\n",
       "        )\n",
       "        (15): GroupNorm(8, 8, eps=1e-06, affine=True)\n",
       "        (16): Convolution(\n",
       "          (conv): Conv2d(8, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (quant_conv_mu): Convolution(\n",
       "      (conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (quant_conv_log_sigma): Convolution(\n",
       "      (conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (post_quant_conv): Convolution(\n",
       "      (conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (H): ProjectionHeads(\n",
       "    (mlp_0): Sequential(\n",
       "      (0): Linear(in_features=8, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (mlp_4): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (mlp_7): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (mlp_10): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (mlp_14): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (perceptual_loss): PerceptualLoss(\n",
       "    (perceptual_function): LPIPS(\n",
       "      (scaling_layer): ScalingLayer()\n",
       "      (net): alexnet(\n",
       "        (slice1): Sequential(\n",
       "          (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (slice2): Sequential(\n",
       "          (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "          (4): ReLU(inplace=True)\n",
       "        )\n",
       "        (slice3): Sequential(\n",
       "          (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "        (slice4): Sequential(\n",
       "          (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (9): ReLU(inplace=True)\n",
       "        )\n",
       "        (slice5): Sequential(\n",
       "          (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (11): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (lin0): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (lin1): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(192, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (lin2): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(384, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (lin3): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (lin4): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (lins): ModuleList(\n",
       "        (0): NetLinLayer(\n",
       "          (model): Sequential(\n",
       "            (0): Dropout(p=0.5, inplace=False)\n",
       "            (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): NetLinLayer(\n",
       "          (model): Sequential(\n",
       "            (0): Dropout(p=0.5, inplace=False)\n",
       "            (1): Conv2d(192, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (2): NetLinLayer(\n",
       "          (model): Sequential(\n",
       "            (0): Dropout(p=0.5, inplace=False)\n",
       "            (1): Conv2d(384, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3-4): 2 x NetLinLayer(\n",
       "          (model): Sequential(\n",
       "            (0): Dropout(p=0.5, inplace=False)\n",
       "            (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (adv_loss): PatchAdversarialLoss(\n",
       "    (activation): LeakyReLU(negative_slope=0.05)\n",
       "    (loss_fct): MSELoss()\n",
       "  )\n",
       "  (nce_loss): NCELoss(\n",
       "    (criterion): CrossEntropyLoss()\n",
       "  )\n",
       "  (l1): L1Loss()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c614757-312e-4474-a5be-c236517bf358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
