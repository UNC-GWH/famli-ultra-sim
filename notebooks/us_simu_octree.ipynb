{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e45f5-bc1e-4650-b328-2e443fbb883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import nrrd\n",
    "import vtk\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import pickle\n",
    "import monai \n",
    "import glob \n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "sys.path.append('/mnt/famli_netapp_shared/C1_ML_Analysis/src/famli-ultra-sim/')\n",
    "sys.path.append('/mnt/famli_netapp_shared/C1_ML_Analysis/src/famli-ultra-sim/dl')\n",
    "import dl.transforms.ultrasound_transforms as ultrasound_transforms\n",
    "import dl.loaders.mr_us_dataset as mr_us_dataset\n",
    "import dl.nets.us_simulation_jit as us_simulation_jit\n",
    "import dl.nets.us_simu as us_simu\n",
    "\n",
    "import importlib\n",
    "\n",
    "from dl.nets.layers import TimeDistributed\n",
    "\n",
    "\n",
    "import ocnn\n",
    "from ocnn.octree import Octree, Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3bc806",
   "metadata": {},
   "outputs": [],
   "source": [
    "mount_point = '/mnt/raid/C1_ML_Analysis'\n",
    "\n",
    "importlib.reload(us_simu)\n",
    "vs = us_simu.VolumeSamplingBlindSweep(mount_point=mount_point, simulation_fov_grid_size=[128, 256, 256])\n",
    "vs.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec6cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# diffusor = sitk.ReadImage('/mnt/famli_netapp_shared/C1_ML_Analysis/src/blender/simulated_data_export/studies_merged/FAM-025-0447-5.nrrd')\n",
    "# diffusor_np = sitk.GetArrayFromImage(diffusor)\n",
    "# diffusor_t = torch.tensor(diffusor_np.astype(int))\n",
    "\n",
    "# diffusor_spacing = torch.tensor(diffusor.GetSpacing()).flip(dims=[0])\n",
    "# diffusor_size = torch.tensor(diffusor.GetSize()).flip(dims=[0])\n",
    "\n",
    "# diffusor_origin = torch.tensor(diffusor.GetOrigin()).flip(dims=[0])\n",
    "# diffusor_end = diffusor_origin + diffusor_spacing * diffusor_size\n",
    "# print(diffusor_size)\n",
    "# print(diffusor_spacing)\n",
    "# print(diffusor_t.shape)\n",
    "# print(diffusor_origin)\n",
    "# print(diffusor_end)\n",
    "\n",
    "diffusor_np, diffusor_head = nrrd.read('/mnt/raid//C1_ML_Analysis/simulated_data_export/placenta/FAM-025-0664-4_label11_resampled.nrrd')\n",
    "diffusor_t = torch.tensor(diffusor_np.astype(int)).permute(2, 1, 0)\n",
    "print(diffusor_head)\n",
    "diffusor_size = torch.tensor(diffusor_head['sizes'])\n",
    "diffusor_spacing = torch.tensor(np.diag(diffusor_head['space directions']))\n",
    "\n",
    "diffusor_origin = torch.tensor(diffusor_head['space origin']).flip(dims=[0])\n",
    "diffusor_end = diffusor_origin + diffusor_spacing * diffusor_size\n",
    "print(diffusor_spacing)\n",
    "print(diffusor_t.shape)\n",
    "print(diffusor_origin)\n",
    "print(diffusor_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d81c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.imshow(diffusor_t.flip(dims=[1]).squeeze().cpu().numpy(), animation_frame=0, binary_string=True)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bae68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusor_batch_t = diffusor_t.permute([2, 1, 0]).cuda().float().unsqueeze(0).unsqueeze(0).repeat(3, 1, 1, 1, 1)\n",
    "# print(diffusor_batch_t.shape)\n",
    "\n",
    "# diffusor_origin_batch = diffusor_origin[None, :].repeat(3, 1) + torch.randn(3, 3) * 0.01\n",
    "# diffusor_end_batch = diffusor_end[None, :].repeat(3, 1) + + torch.randn(3, 3) * 0.01\n",
    "# print(diffusor_origin, diffusor_origin_batch)\n",
    "# # print(diffusor_origin_batch.shape)\n",
    "\n",
    "# diffusor_in_fov_t = vs.diffusor_in_fov(diffusor_batch_t, diffusor_origin_batch.cuda(), diffusor_end_batch.cuda())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c334ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.imshow(diffusor_in_fov_t[0].squeeze().flip(dims=[1]).cpu().numpy(), animation_frame=0, binary_string=True)\n",
    "# fig.show()\n",
    "# fig = px.imshow(diffusor_in_fov_t[1].squeeze().flip(dims=[1]).cpu().numpy(), animation_frame=0, binary_string=True)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24597d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(us_simulation_jit)\n",
    "us_simulator_cut = us_simulation_jit.MergedCutLabel11()\n",
    "grid, inverse_grid, mask_fan = us_simulator_cut.init_grids(256, 256, 128.0, -30.0, 20.0, 215.0, 0.7853981633974483)\n",
    "us_simulator_cut_td = TimeDistributed(us_simulator_cut, time_dim=2).eval().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e44534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for tag in vs.tags:\n",
    "\n",
    "batch_size = 1\n",
    "diffusor_batch_t = diffusor_t.permute([2, 1, 0]).cuda().float().unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1, 1, 1)\n",
    "print(diffusor_batch_t.shape)\n",
    "\n",
    "diffusor_origin_batch = diffusor_origin[None, :].repeat(batch_size, 1) + torch.randn(batch_size, 3) * 0.01\n",
    "diffusor_end_batch = diffusor_end[None, :].repeat(batch_size, 1) + + torch.randn(batch_size, 3) * 0.01\n",
    "\n",
    "out_fovs_list = []\n",
    "for tag in [\"M\", \"L0\", \"C1\"]:\n",
    "        \n",
    "        # print(simulation_ultrasound_plane_mesh_grid_transformed_t_idx.shape)\n",
    "        # print(diffusor_t.shape)\n",
    "\n",
    "        use_random = False\n",
    "        probe_origin_rand = None\n",
    "        probe_direction_rand = None\n",
    "\n",
    "        if use_random:\n",
    "\n",
    "                probe_origin_rand = torch.rand(3)*0.001\n",
    "                probe_origin_rand = probe_origin_rand.cuda()\n",
    "                rotation_ranges = ((-15, 15), (-15, 15), (-30, 30))  # ranges in degrees for x, y, and z rotations\n",
    "                probe_direction_rand = vs.random_affine_matrix(rotation_ranges).cuda()\n",
    "\n",
    "        sampled_sweep = vs.diffusor_sampling_tag(tag, diffusor_batch_t, diffusor_origin_batch.cuda().to(torch.float), diffusor_end_batch.cuda().to(torch.float), probe_origin_rand=probe_origin_rand, probe_direction_rand=probe_direction_rand, use_random=use_random)\n",
    "        with torch.no_grad():\n",
    "                sampled_sweep_simu = torch.cat([us_simulator_cut_td(ss.unsqueeze(dim=0), grid.cuda(), inverse_grid.cuda(), mask_fan.cuda()) for ss in sampled_sweep], dim=0)\n",
    "\n",
    "        # print(sampled_sweep_simu.shape)\n",
    "\n",
    "        out_fovs = vs.simulated_sweep_in_fov(tag, sampled_sweep_simu)\n",
    "        \n",
    "        # print(out_fovs.shape)\n",
    "        out_fovs_list.append(out_fovs)\n",
    "        # print(simulation_ultrasound_plane_mesh_grid_transformed_t.shape)\n",
    "out_fovs = torch.cat(out_fovs_list, dim=0)\n",
    "print(out_fovs.shape)\n",
    "# fig = px.imshow(sampled_sweep[0].squeeze().cpu().numpy(), animation_frame=0, binary_string=True)\n",
    "# fig.show()\n",
    "# fig = px.imshow(sampled_sweep_simu[0].squeeze().cpu().numpy(), animation_frame=0, binary_string=True)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878c5748",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(out_fovs[0].flip(dims=[0]).squeeze().cpu().numpy(), animation_frame=1, binary_string=True)\n",
    "# fig = px.imshow(out_fovs[2].flip(dims=[1]).squeeze().detach().cpu().numpy(), animation_frame=0, binary_string=True)\n",
    "# fig = px.imshow(out_fovs[2].squeeze().detach().cpu().numpy(), animation_frame=0, binary_string=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0099eb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fov_physical = vs.transform_fov_norm(vs.fov_physical())\n",
    "\n",
    "# repeats = [1,]*len(out_fovs.shape)\n",
    "# repeats[0] = out_fovs.shape[0]\n",
    "\n",
    "# fov_physical = fov_physical.repeat(repeats)\n",
    "\n",
    "V = fov_physical.reshape(-1, 3).cuda()\n",
    "\n",
    "octrees = []\n",
    "points = []\n",
    "\n",
    "for sweep_in_fov in out_fovs:\n",
    "\n",
    "        sweep_in_fov = sweep_in_fov.reshape(-1, 1)\n",
    "\n",
    "        octree = Octree(7)\n",
    "        \n",
    "        V_filtered = V[sweep_in_fov.squeeze() > 0.1]\n",
    "        sweep_in_fov_filtered = sweep_in_fov[sweep_in_fov.squeeze() > 0.1]\n",
    "        p = Points(V_filtered, features=sweep_in_fov_filtered)\n",
    "        # p = Points(V, features=sweep_in_fov)\n",
    "\n",
    "        octree.build_octree(p)\n",
    "        \n",
    "        octrees.append(octree)\n",
    "        points.append(p)\n",
    "\n",
    "\n",
    "points = ocnn.octree.merge_points(points)\n",
    "octree = ocnn.octree.merge_octrees(octrees)\n",
    "# NOTE: remember to construct the neighbor indices\n",
    "octree.construct_all_neigh()\n",
    "\n",
    "# octree_1 = Octree(16)\n",
    "# points_1 = Points(V, features=)\n",
    "# octree_1.build_octree(points_1)\n",
    "# out_fovs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e4cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x, y, z, b = octree.xyzb(7)\n",
    "aw = torch.argwhere(b == 2).squeeze()\n",
    "\n",
    "x = x[aw]\n",
    "y = y[aw]\n",
    "z = z[aw]\n",
    "feat = octree.get_input_feature('F')[aw]\n",
    "\n",
    "N = 100000\n",
    "random_indices = torch.randperm(x.size(0))[:N]\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(x=z[random_indices].cpu().numpy(), y=y[random_indices].cpu().numpy(), z=x[random_indices].cpu().numpy(), mode='markers', marker=dict(\n",
    "        size=2,\n",
    "        color=feat[random_indices].cpu().numpy().squeeze(),                # set color to an array/list of desired values\n",
    "        colorscale='jet',   # choose a colorscale\n",
    "        opacity=0.5\n",
    "    ))])\n",
    "fig.show()\n",
    "print(torch.min(octree.get_input_feature('F')[random_indices]), torch.max(octree.get_input_feature('F')[random_indices]))\n",
    "print(octree.get_input_feature('F')[random_indices].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6118d1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusor_plane_t = diffusor_plane_t.squeeze().unsqueeze(1)\n",
    "\n",
    "# diffusor_plane_t = vs.diffusor_sampling_tag('C1', diffusor_t.unsqueeze(0).unsqueeze(0).cuda().to(torch.float), diffusor_origin.cuda().to(torch.float), diffusor_end.cuda().to(torch.float))\n",
    "\n",
    "# # print(diffusor_plane_t.shape)\n",
    "# repeats = [1,]*4\n",
    "# repeats[0] = diffusor_plane_t.shape[0]\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     x = us_simulator_cut_td(diffusor_plane_t, grid.repeat(repeats).cuda(), inverse_grid.repeat(repeats).cuda(), mask_fan.repeat(repeats).cuda())\n",
    "\n",
    "# fig = px.imshow(x.squeeze().cpu().numpy(), animation_frame=0, binary_string=True)\n",
    "# fig.show()\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# x, y, z, b = octree.xyzb(7)\n",
    "# print(x.shape)\n",
    "# print(octree.get_input_feature('F').shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d20d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = ocnn.models.UNet(in_channels=4, out_channels=3).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5464d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = octree.get_input_feature('PF').to(torch.float)\n",
    "query_pts = torch.cat([points.points, points.batch_id], dim=1)\n",
    "\n",
    "logit = unet(data, octree, octree.depth, query_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab7652",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ac99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ocnn.utils import meshgrid, scatter_add, resize_with_last_val, list2str\n",
    "from typing import List\n",
    "\n",
    "class OctreePoolBase(torch.nn.Module):\n",
    "  r''' The base class for octree-based pooling.\n",
    "  '''\n",
    "\n",
    "  def __init__(self, kernel_size: List[int], stride: int, nempty: bool = False):\n",
    "    super().__init__()\n",
    "    self.kernel_size = resize_with_last_val(kernel_size)\n",
    "    self.kernel = list2str(self.kernel_size)\n",
    "    self.stride = stride\n",
    "    self.nempty = nempty\n",
    "\n",
    "  def extra_repr(self) -> str:\n",
    "    return ('kernel_size={}, stride={}, nempty={}').format(\n",
    "            self.kernel_size, self.stride, self.nempty)  # noqa\n",
    "\n",
    "\n",
    "def octree_attn_pool(data: torch.Tensor, octree: Octree, depth: int,\n",
    "                    kernel: str, stride: int = 2, nempty: bool = False):\n",
    "  r''' Performs octree attention pooling.\n",
    "\n",
    "  Args:\n",
    "    data (torch.Tensor): The input tensor.\n",
    "    octree (Octree): The corresponding octree.\n",
    "    depth (int): The depth of current octree.\n",
    "    kernel (str): The kernel size, like '333', '222'.\n",
    "    stride (int): The stride of the pooling.\n",
    "    nempty (bool): If True, :attr:`data` contains only features of non-empty\n",
    "        octree nodes.\n",
    "  '''\n",
    "\n",
    "  neigh = octree.get_neigh(depth, kernel, stride, nempty)\n",
    "\n",
    "  N1 = data.shape[0]\n",
    "  N2 = neigh.shape[0]\n",
    "  K = neigh.shape[1]\n",
    "\n",
    "  mask = neigh >= 0\n",
    "  val = 1.0 / (torch.sum(mask, dim=1) + 1e-8)\n",
    "  \n",
    "  mask = mask.view(-1)\n",
    "  val = val.unsqueeze(1).repeat(1, K).reshape(-1)\n",
    "  val = val[mask]\n",
    "  \n",
    "\n",
    "  row = torch.arange(N2, device=neigh.device)\n",
    "  row = row.unsqueeze(1).repeat(1, K).view(-1)\n",
    "  col = neigh.view(-1)\n",
    "  indices = torch.stack([row[mask], col[mask]], dim=0).long()\n",
    "\n",
    "  mat = torch.sparse_coo_tensor(indices, val, [N2, N1], device=data.device)\n",
    "  out = torch.sparse.mm(mat, data)\n",
    "  return out\n",
    "\n",
    "class OctreeAttnPool(OctreePoolBase):\n",
    "  r''' Performs octree average pooling.\n",
    "\n",
    "  Please refer to :func:`octree_avg_pool` for details.\n",
    "  '''\n",
    "\n",
    "  def forward(self, data: torch.Tensor, octree: Octree, depth: int):\n",
    "    r''''''\n",
    "\n",
    "    return octree_attn_pool(\n",
    "        data, octree, depth, self.kernel, self.stride, self.nempty)\n",
    "\n",
    "attn_pool = OctreeAttnPool(kernel_size=[3,3,3], stride=2).cuda()\n",
    "\n",
    "print(attn_pool(data, octree, octree.depth).shape)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6787172",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/mnt/famli_netapp_shared/C1_ML_Analysis/src/ShapeAXI')\n",
    "from shapeaxi.saxi_layers import Residual, FeedForward\n",
    "\n",
    "class OctreeMHA(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1, return_weights=False, use_direction=True, kernel_size=[3,3,3], nempty=False):\n",
    "        super(OctreeMHA, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.return_weights = return_weights\n",
    "        self.kernel = list2str(resize_with_last_val(kernel_size))\n",
    "        self.nempty = nempty\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, bias=False, batch_first=True)\n",
    "        self.use_direction = use_direction\n",
    "    \n",
    "    def forward(self, x, octree, depth):\n",
    "\n",
    "        # batch_size, V_n, Embed_dim = x.shape\n",
    "\n",
    "        neigh = octree.get_neigh(depth, kernel=self.kernel, stride=1, nempty=self.nempty)\n",
    "\n",
    "        # the query is the input point itself, the shape of q is [BS, V_n, 1, Embed_dim]\n",
    "        q = x.unsqueeze(-2)\n",
    "        \n",
    "        k = x[neigh]\n",
    "        \n",
    "        #the value tensor contains the directions towards the closest points. \n",
    "        # the intuition here is that based on the query and key embeddings, the model will learn to predict\n",
    "        # the best direction to move the new embedding, i.e., create a new point in the point cloud\n",
    "        # the shape of v is [BS, V_n, K, Embed_dim]\n",
    "        if self.use_direction:\n",
    "            v = k - q\n",
    "        else:\n",
    "            v = k\n",
    "\n",
    "        v, x_w = self.attention(q, k, v)\n",
    "        v = v.squeeze(-2)\n",
    "        # v = v.contiguous().view(batch_size, V_n, Embed_dim)\n",
    "        # x_w = x_w.contiguous().view(batch_size, V_n, self.K)\n",
    "\n",
    "        # x_w = torch.zeros(batch_size, V_n, device=x.device).scatter_add_(1, dists.idx.view(batch_size, -1), x_w.view(batch_size, -1))\n",
    "        \n",
    "        # The new predicted point is the sum of the input point and the weighted sum of the directions\n",
    "        if self.use_direction:\n",
    "            x = x + v\n",
    "        else:\n",
    "            x = v\n",
    "        \n",
    "        if self.return_weights:\n",
    "            return x, x_w\n",
    "        return x\n",
    "\n",
    "nempty=True\n",
    "# print(OctreeMHA(4, 4, use_direction=False, nempty=nempty).cuda()(octree.get_input_feature('PF', nempty=nempty).to(torch.float), octree, octree.depth).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a9158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OctreeMHAEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=3, embed_dim=8, hidden_dim=64, kernel_size=[3,3,3], num_heads=8, output_dim=256, stages=[8, 16, 32, 64, 128], dropout=0.1):\n",
    "        super(OctreeMHAEncoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        embed_dim = self.embed_dim\n",
    "        for i, st in enumerate(stages):\n",
    "            setattr(self, f\"octree_mha{i}\", OctreeMHA(embed_dim=embed_dim, num_heads=num_heads, return_weights=True, dropout=dropout, use_direction=True, kernel_size=[3,3,3], nempty=False))\n",
    "            setattr(self, f\"ff_{i}\", Residual(FeedForward(embed_dim, hidden_dim=hidden_dim, dropout=dropout)))\n",
    "            setattr(self, f\"pool_{i}\", OctreeAttnPool(kernel_size=[3,3,3], stride=2))\n",
    "\n",
    "        \n",
    "        self.output = nn.Linear(embed_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "\n",
    "        weights = torch.zeros(x.shape[0], x.shape[1], device=x.device)\n",
    "        idx = torch.arange(x.shape[1], device=x.device).unsqueeze(0).expand(x.shape[0], -1)\n",
    "        \n",
    "        indices = []\n",
    "        \n",
    "        for i, sl in enumerate(self.sample_levels):\n",
    "            \n",
    "            if i > 0:\n",
    "                # select the first sl points a.k.a. downsample/pooling                \n",
    "                x, x_i = self.sample_points(x, sl)\n",
    "\n",
    "                # initialize idx with the index of the current level\n",
    "                idx = x_i\n",
    "                \n",
    "                for idx_prev in reversed(indices): # go through the list of the previous ones in reverse\n",
    "                    idx = knn_gather(idx_prev, idx).squeeze(-2).contiguous() # using the indices of the previous level update idx, at the end idx should have the indices of the first level\n",
    "                \n",
    "                idx = idx.squeeze(-1)\n",
    "                indices.append(x_i)\n",
    "            \n",
    "            # the mha will select optimal points from the input\n",
    "            x, x_w = getattr(self, f\"mha_{i}\")(x)\n",
    "            x = getattr(self, f\"ff_{i}\")(x)\n",
    "            \n",
    "            weights.scatter_add_(1, idx, x_w)\n",
    "\n",
    "        #output layer\n",
    "        x = self.output(x)\n",
    "        return x, weights\n",
    "    \n",
    "# OctreeMHAEncoder(input_dim=4, embed_dim=128, hidden_dim=64, kernel_size=[3,3,3], num_heads=8, output_dim=256, stages=[8, 16, 32, 64, 128], dropout=0.1).cuda()(octree.get_input_feature('PF').to(torch.float)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9617615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
